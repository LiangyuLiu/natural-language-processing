{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n",
      "Downloading GoogleNews-vectors-negative300.bin.gz (1.5G) for you, it will take a while...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcf48962aa54efea1bc90e27676dcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1647046227), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with gzip.open('GoogleNews-vectors-negative300.bin.gz', 'rb') as f_in:\n",
    "    with open('GoogleNews-vectors-negative300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "wv_embeddings = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py:572: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    q_embedding = np.zeros(dim)\n",
    "    if question == \"\":\n",
    "        return q_embedding\n",
    "\n",
    "    n_words = 0\n",
    "    words = question.split()\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            q_embedding += embeddings[word]\n",
    "            n_words += 1\n",
    "    if n_words > 1:\n",
    "        q_embedding /= n_words\n",
    "    return q_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    count = 0\n",
    "    for d in dup_ranks:\n",
    "        if d <= k:\n",
    "            count += 1\n",
    "    return count / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    count = 0\n",
    "    for d in dup_ranks:\n",
    "        if d <= k:\n",
    "            count += 1 / math.log(1 + d, 2)\n",
    "    return count / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857287\n",
      "0.5\n",
      "0.8154648767857287\n",
      "0.3333333333333333\n",
      "0.5436432511904857\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, candidate)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    question_embedding = question_to_vec(question, embeddings, dim).reshape(1, -1)\n",
    "    candidate_embeddings = np.array([question_to_vec(c, embeddings, dim) for c in candidates])\n",
    "    distances = cosine_similarity(candidate_embeddings, question_embedding)\n",
    "    indices = np.argsort(distances[:,0])[::-1]\n",
    "    return [(i, candidates[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.209 | Hits@   1: 0.209\n",
      "DCG@   5: 0.263 | Hits@   5: 0.311\n",
      "DCG@  10: 0.279 | Hits@  10: 0.360\n",
      "DCG@ 100: 0.316 | Hits@ 100: 0.548\n",
      "DCG@ 500: 0.349 | Hits@ 500: 0.807\n",
      "DCG@1000: 0.369 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1001\n"
     ]
    }
   ],
   "source": [
    "print(len(validation), len(validation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    ######### YOUR CODE HERE #############\n",
    "    prepared_validation.append([text_prepare(q) for q in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.305 | Hits@   1: 0.305\n",
      "DCG@   5: 0.375 | Hits@   5: 0.438\n",
      "DCG@  10: 0.392 | Hits@  10: 0.489\n",
      "DCG@ 100: 0.425 | Hits@ 100: 0.656\n",
      "DCG@ 500: 0.447 | Hits@ 500: 0.830\n",
      "DCG@1000: 0.465 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('data/train.tsv', 'data/train_prepared.tsv')\n",
    "prepare_file('data/test.tsv', 'data/test_prepared.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "######### YOUR CODE HERE #############\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training model. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enough for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61.0%  lr: 0.044144  loss: 0.010859  eta: 0h14m  tot: 0h1m59s  (12.2%)%  lr: 0.049910  loss: 0.040950  eta: 0h17m  tot: 0h0m3s  (0.3%)2.5%  lr: 0.049790  loss: 0.036531  eta: 0h17m  tot: 0h0m5s  (0.5%)2.8%  lr: 0.049780  loss: 0.034615  eta: 0h17m  tot: 0h0m5s  (0.6%)4.1%  lr: 0.049580  loss: 0.029741  eta: 0h16m  tot: 0h0m8s  (0.8%)4.4%  lr: 0.049570  loss: 0.029028  eta: 0h16m  tot: 0h0m8s  (0.9%)4.5%  lr: 0.049540  loss: 0.028873  eta: 0h16m  tot: 0h0m9s  (0.9%)0h0m10s  (1.1%)%  lr: 0.049449  loss: 0.026117  eta: 0h17m  tot: 0h0m12s  (1.2%)6.9%  lr: 0.049319  loss: 0.024432  eta: 0h17m  tot: 0h0m14s  (1.4%)8.3%  lr: 0.049209  loss: 0.022611  eta: 0h16m  tot: 0h0m16s  (1.7%)0m17s  (1.7%)8.9%  lr: 0.049119  loss: 0.022082  eta: 0h16m  tot: 0h0m18s  (1.8%)9.3%  lr: 0.049109  loss: 0.021752  eta: 0h16m  tot: 0h0m18s  (1.9%)9.5%  lr: 0.049079  loss: 0.021567  eta: 0h16m  tot: 0h0m19s  (1.9%)9.9%  lr: 0.049039  loss: 0.021171  eta: 0h16m  tot: 0h0m20s  (2.0%)10.1%  lr: 0.049009  loss: 0.020944  eta: 0h16m  tot: 0h0m20s  (2.0%)10.3%  lr: 0.048959  loss: 0.020808  eta: 0h17m  tot: 0h0m21s  (2.1%)10.7%  lr: 0.048929  loss: 0.020593  eta: 0h17m  tot: 0h0m22s  (2.1%)11.2%  lr: 0.048869  loss: 0.020253  eta: 0h17m  tot: 0h0m23s  (2.2%) (2.3%)11.6%  lr: 0.048819  loss: 0.020061  eta: 0h17m  tot: 0h0m24s  (2.3%)12.6%  lr: 0.048769  loss: 0.019483  eta: 0h17m  tot: 0h0m26s  (2.5%)13.2%  lr: 0.048679  loss: 0.019392  eta: 0h16m  tot: 0h0m27s  (2.6%)13.4%  lr: 0.048659  loss: 0.019273  eta: 0h16m  tot: 0h0m28s  (2.7%)13.5%  lr: 0.048659  loss: 0.019223  eta: 0h16m  tot: 0h0m28s  (2.7%)13.7%  lr: 0.048659  loss: 0.019109  eta: 0h16m  tot: 0h0m28s  (2.7%)14.0%  lr: 0.048629  loss: 0.018916  eta: 0h16m  tot: 0h0m29s  (2.8%)14.3%  lr: 0.048599  loss: 0.018809  eta: 0h16m  tot: 0h0m29s  (2.9%)14.7%  lr: 0.048549  loss: 0.018712  eta: 0h16m  tot: 0h0m30s  (2.9%)15.6%  lr: 0.048438  loss: 0.018233  eta: 0h17m  tot: 0h0m33s  (3.1%)15.8%  lr: 0.048388  loss: 0.018086  eta: 0h17m  tot: 0h0m34s  (3.2%)16.0%  lr: 0.048358  loss: 0.018003  eta: 0h17m  tot: 0h0m34s  (3.2%)16.7%  lr: 0.048268  loss: 0.017811  eta: 0h17m  tot: 0h0m36s  (3.3%)17.3%  lr: 0.048198  loss: 0.017553  eta: 0h17m  tot: 0h0m37s  (3.5%)17.7%  lr: 0.048118  loss: 0.017360  eta: 0h17m  tot: 0h0m38s  (3.5%)17.8%  lr: 0.048118  loss: 0.017315  eta: 0h17m  tot: 0h0m38s  (3.6%)17.9%  lr: 0.048118  loss: 0.017273  eta: 0h17m  tot: 0h0m39s  (3.6%)18.5%  lr: 0.048068  loss: 0.017055  eta: 0h17m  tot: 0h0m40s  (3.7%)19.4%  lr: 0.048008  loss: 0.016808  eta: 0h17m  tot: 0h0m42s  (3.9%)19.7%  lr: 0.047988  loss: 0.016746  eta: 0h17m  tot: 0h0m42s  (3.9%)20.5%  lr: 0.047938  loss: 0.016477  eta: 0h17m  tot: 0h0m44s  (4.1%)21.1%  lr: 0.047888  loss: 0.016287  eta: 0h17m  tot: 0h0m45s  (4.2%)22.0%  lr: 0.047818  loss: 0.016022  eta: 0h17m  tot: 0h0m47s  (4.4%)22.3%  lr: 0.047768  loss: 0.015916  eta: 0h17m  tot: 0h0m47s  (4.5%)23.0%  lr: 0.047688  loss: 0.015710  eta: 0h16m  tot: 0h0m48s  (4.6%)23.3%  lr: 0.047668  loss: 0.015599  eta: 0h16m  tot: 0h0m49s  (4.7%)23.8%  lr: 0.047598  loss: 0.015489  eta: 0h16m  tot: 0h0m50s  (4.8%)24.1%  lr: 0.047598  loss: 0.015383  eta: 0h16m  tot: 0h0m51s  (4.8%)24.6%  lr: 0.047538  loss: 0.015285  eta: 0h16m  tot: 0h0m51s  (4.9%)25.0%  lr: 0.047508  loss: 0.015164  eta: 0h16m  tot: 0h0m52s  (5.0%)25.4%  lr: 0.047468  loss: 0.015036  eta: 0h16m  tot: 0h0m53s  (5.1%)27.1%  lr: 0.047357  loss: 0.014657  eta: 0h16m  tot: 0h0m56s  (5.4%)28.0%  lr: 0.047247  loss: 0.014473  eta: 0h16m  tot: 0h0m57s  (5.6%)28.2%  lr: 0.047227  loss: 0.014433  eta: 0h16m  tot: 0h0m58s  (5.6%)0h16m  tot: 0h0m59s  (5.8%)30.4%  lr: 0.047117  loss: 0.014114  eta: 0h16m  tot: 0h1m2s  (6.1%)30.6%  lr: 0.047077  loss: 0.014084  eta: 0h16m  tot: 0h1m2s  (6.1%)31.0%  lr: 0.047067  loss: 0.014013  eta: 0h16m  tot: 0h1m3s  (6.2%)31.3%  lr: 0.047047  loss: 0.013932  eta: 0h15m  tot: 0h1m4s  (6.3%)31.5%  lr: 0.047037  loss: 0.013908  eta: 0h15m  tot: 0h1m4s  (6.3%)32.0%  lr: 0.047017  loss: 0.013808  eta: 0h15m  tot: 0h1m5s  (6.4%)32.6%  lr: 0.046957  loss: 0.013718  eta: 0h15m  tot: 0h1m6s  (6.5%)33.1%  lr: 0.046927  loss: 0.013622  eta: 0h15m  tot: 0h1m7s  (6.6%)33.7%  lr: 0.046897  loss: 0.013533  eta: 0h15m  tot: 0h1m8s  (6.7%)%)%  lr: 0.046827  loss: 0.013455  eta: 0h15m  tot: 0h1m9s  (6.8%)34.4%  lr: 0.046817  loss: 0.013432  eta: 0h15m  tot: 0h1m10s  (6.9%)35.2%  lr: 0.046697  loss: 0.013349  eta: 0h15m  tot: 0h1m11s  (7.0%)36.1%  lr: 0.046597  loss: 0.013194  eta: 0h15m  tot: 0h1m13s  (7.2%)36.3%  lr: 0.046587  loss: 0.013186  eta: 0h15m  tot: 0h1m13s  (7.3%)36.7%  lr: 0.046517  loss: 0.013127  eta: 0h15m  tot: 0h1m14s  (7.3%)36.8%  lr: 0.046507  loss: 0.013110  eta: 0h15m  tot: 0h1m14s  (7.4%)37.2%  lr: 0.046487  loss: 0.013059  eta: 0h15m  tot: 0h1m15s  (7.4%)%  lr: 0.046236  loss: 0.012908  eta: 0h15m  tot: 0h1m18s  (7.7%)38.8%  lr: 0.046226  loss: 0.012900  eta: 0h15m  tot: 0h1m18s  (7.8%)39.5%  lr: 0.046216  loss: 0.012836  eta: 0h15m  tot: 0h1m19s  (7.9%)39.7%  lr: 0.046206  loss: 0.012815  eta: 0h15m  tot: 0h1m20s  (7.9%)40.3%  lr: 0.046176  loss: 0.012726  eta: 0h15m  tot: 0h1m21s  (8.1%)40.4%  lr: 0.046176  loss: 0.012713  eta: 0h15m  tot: 0h1m21s  (8.1%)40.7%  lr: 0.046146  loss: 0.012677  eta: 0h15m  tot: 0h1m22s  (8.1%)42.4%  lr: 0.045906  loss: 0.012471  eta: 0h15m  tot: 0h1m25s  (8.5%)43.9%  lr: 0.045756  loss: 0.012280  eta: 0h15m  tot: 0h1m28s  (8.8%)44.0%  lr: 0.045756  loss: 0.012272  eta: 0h15m  tot: 0h1m28s  (8.8%)44.1%  lr: 0.045746  loss: 0.012263  eta: 0h15m  tot: 0h1m28s  (8.8%)44.8%  lr: 0.045626  loss: 0.012197  eta: 0h15m  tot: 0h1m30s  (9.0%)45.3%  lr: 0.045616  loss: 0.012145  eta: 0h15m  tot: 0h1m30s  (9.1%)45.4%  lr: 0.045606  loss: 0.012134  eta: 0h15m  tot: 0h1m31s  (9.1%)46.2%  lr: 0.045556  loss: 0.012048  eta: 0h15m  tot: 0h1m32s  (9.2%)46.4%  lr: 0.045556  loss: 0.012031  eta: 0h15m  tot: 0h1m32s  (9.3%)46.6%  lr: 0.045526  loss: 0.012001  eta: 0h15m  tot: 0h1m33s  (9.3%)46.8%  lr: 0.045506  loss: 0.011987  eta: 0h15m  tot: 0h1m33s  (9.4%)47.4%  lr: 0.045436  loss: 0.011932  eta: 0h15m  tot: 0h1m34s  (9.5%)47.7%  lr: 0.045415  loss: 0.011899  eta: 0h15m  tot: 0h1m35s  (9.5%)47.9%  lr: 0.045395  loss: 0.011878  eta: 0h15m  tot: 0h1m35s  (9.6%)48.0%  lr: 0.045395  loss: 0.011864  eta: 0h15m  tot: 0h1m35s  (9.6%)48.5%  lr: 0.045305  loss: 0.011822  eta: 0h14m  tot: 0h1m36s  (9.7%)48.7%  lr: 0.045275  loss: 0.011814  eta: 0h14m  tot: 0h1m36s  (9.7%)49.6%  lr: 0.045205  loss: 0.011725  eta: 0h14m  tot: 0h1m38s  (9.9%)49.9%  lr: 0.045165  loss: 0.011695  eta: 0h14m  tot: 0h1m38s  (10.0%)50.2%  lr: 0.045145  loss: 0.011670  eta: 0h14m  tot: 0h1m39s  (10.0%)50.3%  lr: 0.045135  loss: 0.011658  eta: 0h14m  tot: 0h1m39s  (10.1%)50.6%  lr: 0.045115  loss: 0.011628  eta: 0h14m  tot: 0h1m40s  (10.1%)51.0%  lr: 0.045085  loss: 0.011599  eta: 0h14m  tot: 0h1m40s  (10.2%)51.4%  lr: 0.044995  loss: 0.011553  eta: 0h14m  tot: 0h1m41s  (10.3%)51.8%  lr: 0.044975  loss: 0.011532  eta: 0h14m  tot: 0h1m42s  (10.4%)51.9%  lr: 0.044965  loss: 0.011516  eta: 0h14m  tot: 0h1m42s  (10.4%)52.2%  lr: 0.044955  loss: 0.011496  eta: 0h14m  tot: 0h1m43s  (10.4%)52.6%  lr: 0.044925  loss: 0.011474  eta: 0h14m  tot: 0h1m43s  (10.5%)52.9%  lr: 0.044905  loss: 0.011443  eta: 0h14m  tot: 0h1m44s  (10.6%)53.5%  lr: 0.044855  loss: 0.011407  eta: 0h14m  tot: 0h1m45s  (10.7%)56.0%  lr: 0.044605  loss: 0.011205  eta: 0h14m  tot: 0h1m50s  (11.2%)%  lr: 0.044565  loss: 0.011179  eta: 0h14m  tot: 0h1m51s  (11.3%)57.1%  lr: 0.044535  loss: 0.011146  eta: 0h14m  tot: 0h1m52s  (11.4%)57.3%  lr: 0.044515  loss: 0.011147  eta: 0h14m  tot: 0h1m52s  (11.5%)57.7%  lr: 0.044455  loss: 0.011114  eta: 0h14m  tot: 0h1m53s  (11.5%)58.2%  lr: 0.044384  loss: 0.011078  eta: 0h14m  tot: 0h1m54s  (11.6%)59.1%  lr: 0.044314  loss: 0.011022  eta: 0h14m  tot: 0h1m56s  (11.8%)59.2%  lr: 0.044314  loss: 0.011002  eta: 0h14m  tot: 0h1m56s  (11.8%)59.7%  lr: 0.044274  loss: 0.010967  eta: 0h14m  tot: 0h1m57s  (11.9%)59.9%  lr: 0.044244  loss: 0.010957  eta: 0h14m  tot: 0h1m57s  (12.0%)60.0%  lr: 0.044244  loss: 0.010946  eta: 0h14m  tot: 0h1m57s  (12.0%)  lr: 0.044234  loss: 0.010939  eta: 0h14m  tot: 0h1m57s  (12.0%)61.1%  lr: 0.044114  loss: 0.010856  eta: 0h14m  tot: 0h1m59s  (12.2%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.040090  loss: 0.008985  eta: 0h12m  tot: 0h3m9s  (20.0%)2.5%  lr: 0.043964  loss: 0.010809  eta: 0h14m  tot: 0h2m2s  (12.5%)63.1%  lr: 0.043874  loss: 0.010761  eta: 0h14m  tot: 0h2m3s  (12.6%)64.3%  lr: 0.043684  loss: 0.010686  eta: 0h14m  tot: 0h2m5s  (12.9%)65.3%  lr: 0.043594  loss: 0.010625  eta: 0h14m  tot: 0h2m7s  (13.1%)65.5%  lr: 0.043584  loss: 0.010607  eta: 0h14m  tot: 0h2m8s  (13.1%)65.7%  lr: 0.043584  loss: 0.010598  eta: 0h14m  tot: 0h2m8s  (13.1%)66.2%  lr: 0.043534  loss: 0.010567  eta: 0h14m  tot: 0h2m9s  (13.2%)66.3%  lr: 0.043524  loss: 0.010557  eta: 0h14m  tot: 0h2m9s  (13.3%)66.9%  lr: 0.043434  loss: 0.010518  eta: 0h14m  tot: 0h2m10s  (13.4%)67.6%  lr: 0.043363  loss: 0.010460  eta: 0h14m  tot: 0h2m11s  (13.5%)68.0%  lr: 0.043353  loss: 0.010447  eta: 0h13m  tot: 0h2m12s  (13.6%)68.4%  lr: 0.043263  loss: 0.010414  eta: 0h13m  tot: 0h2m12s  (13.7%)68.6%  lr: 0.043233  loss: 0.010403  eta: 0h13m  tot: 0h2m13s  (13.7%)69.0%  lr: 0.043213  loss: 0.010373  eta: 0h13m  tot: 0h2m13s  (13.8%)69.7%  lr: 0.043163  loss: 0.010331  eta: 0h13m  tot: 0h2m15s  (13.9%)70.2%  lr: 0.043113  loss: 0.010291  eta: 0h13m  tot: 0h2m15s  (14.0%)70.7%  lr: 0.043073  loss: 0.010281  eta: 0h13m  tot: 0h2m16s  (14.1%)71.1%  lr: 0.043043  loss: 0.010257  eta: 0h13m  tot: 0h2m17s  (14.2%)71.2%  lr: 0.043043  loss: 0.010249  eta: 0h13m  tot: 0h2m17s  (14.2%)73.2%  lr: 0.042903  loss: 0.010135  eta: 0h13m  tot: 0h2m21s  (14.6%)73.5%  lr: 0.042833  loss: 0.010119  eta: 0h13m  tot: 0h2m21s  (14.7%)73.9%  lr: 0.042783  loss: 0.010090  eta: 0h13m  tot: 0h2m22s  (14.8%)74.1%  lr: 0.042763  loss: 0.010076  eta: 0h13m  tot: 0h2m23s  (14.8%)74.9%  lr: 0.042683  loss: 0.010034  eta: 0h13m  tot: 0h2m24s  (15.0%)75.4%  lr: 0.042633  loss: 0.010010  eta: 0h13m  tot: 0h2m25s  (15.1%)75.6%  lr: 0.042613  loss: 0.010008  eta: 0h13m  tot: 0h2m25s  (15.1%)75.8%  lr: 0.042613  loss: 0.010000  eta: 0h13m  tot: 0h2m25s  (15.2%)76.0%  lr: 0.042613  loss: 0.009992  eta: 0h13m  tot: 0h2m26s  (15.2%)76.5%  lr: 0.042563  loss: 0.009966  eta: 0h13m  tot: 0h2m27s  (15.3%)  tot: 0h2m29s  (15.5%)78.1%  lr: 0.042463  loss: 0.009888  eta: 0h13m  tot: 0h2m30s  (15.6%)78.3%  lr: 0.042463  loss: 0.009878  eta: 0h13m  tot: 0h2m30s  (15.7%)%  lr: 0.042443  loss: 0.009876  eta: 0h13m  tot: 0h2m30s  (15.7%)78.7%  lr: 0.042433  loss: 0.009860  eta: 0h13m  tot: 0h2m30s  (15.7%)79.2%  lr: 0.042332  loss: 0.009821  eta: 0h13m  tot: 0h2m31s  (15.8%)79.4%  lr: 0.042322  loss: 0.009813  eta: 0h13m  tot: 0h2m32s  (15.9%)79.9%  lr: 0.042272  loss: 0.009794  eta: 0h13m  tot: 0h2m32s  (16.0%)80.2%  lr: 0.042262  loss: 0.009780  eta: 0h13m  tot: 0h2m33s  (16.0%)80.6%  lr: 0.042252  loss: 0.009763  eta: 0h13m  tot: 0h2m33s  (16.1%)80.8%  lr: 0.042222  loss: 0.009751  eta: 0h13m  tot: 0h2m34s  (16.2%)81.1%  lr: 0.042192  loss: 0.009734  eta: 0h13m  tot: 0h2m34s  (16.2%)81.7%  lr: 0.042092  loss: 0.009706  eta: 0h13m  tot: 0h2m36s  (16.3%)81.9%  lr: 0.042062  loss: 0.009703  eta: 0h13m  tot: 0h2m36s  (16.4%)82.1%  lr: 0.042022  loss: 0.009693  eta: 0h13m  tot: 0h2m36s  (16.4%)%  lr: 0.041942  loss: 0.009655  eta: 0h13m  tot: 0h2m38s  (16.6%)83.2%  lr: 0.041922  loss: 0.009655  eta: 0h13m  tot: 0h2m38s  (16.6%)84.3%  lr: 0.041812  loss: 0.009589  eta: 0h13m  tot: 0h2m40s  (16.9%)84.4%  lr: 0.041802  loss: 0.009582  eta: 0h13m  tot: 0h2m40s  (16.9%)84.7%  lr: 0.041752  loss: 0.009564  eta: 0h13m  tot: 0h2m41s  (16.9%)84.9%  lr: 0.041742  loss: 0.009558  eta: 0h13m  tot: 0h2m41s  (17.0%)85.2%  lr: 0.041712  loss: 0.009543  eta: 0h13m  tot: 0h2m42s  (17.0%)86.1%  lr: 0.041592  loss: 0.009511  eta: 0h13m  tot: 0h2m43s  (17.2%)86.5%  lr: 0.041522  loss: 0.009484  eta: 0h13m  tot: 0h2m44s  (17.3%)88.3%  lr: 0.041331  loss: 0.009426  eta: 0h13m  tot: 0h2m47s  (17.7%)88.4%  lr: 0.041301  loss: 0.009425  eta: 0h13m  tot: 0h2m47s  (17.7%)88.6%  lr: 0.041271  loss: 0.009419  eta: 0h13m  tot: 0h2m48s  (17.7%)89.2%  lr: 0.041181  loss: 0.009391  eta: 0h12m  tot: 0h2m49s  (17.8%)89.8%  lr: 0.041151  loss: 0.009362  eta: 0h12m  tot: 0h2m50s  (18.0%)%  lr: 0.041091  loss: 0.009329  eta: 0h12m  tot: 0h2m51s  (18.1%)91.3%  lr: 0.041021  loss: 0.009307  eta: 0h12m  tot: 0h2m52s  (18.3%)91.7%  lr: 0.040981  loss: 0.009296  eta: 0h12m  tot: 0h2m53s  (18.3%)91.8%  lr: 0.040961  loss: 0.009291  eta: 0h12m  tot: 0h2m53s  (18.4%)92.2%  lr: 0.040851  loss: 0.009285  eta: 0h12m  tot: 0h2m54s  (18.4%)18.5%)92.7%  lr: 0.040791  loss: 0.009260  eta: 0h12m  tot: 0h2m55s  (18.5%)93.6%  lr: 0.040731  loss: 0.009224  eta: 0h12m  tot: 0h2m56s  (18.7%)93.9%  lr: 0.040691  loss: 0.009214  eta: 0h12m  tot: 0h2m57s  (18.8%)94.5%  lr: 0.040611  loss: 0.009193  eta: 0h12m  tot: 0h2m59s  (18.9%)95.0%  lr: 0.040601  loss: 0.009183  eta: 0h12m  tot: 0h3m0s  (19.0%)95.5%  lr: 0.040581  loss: 0.009157  eta: 0h12m  tot: 0h3m1s  (19.1%)96.1%  lr: 0.040531  loss: 0.009143  eta: 0h12m  tot: 0h3m2s  (19.2%)96.2%  lr: 0.040471  loss: 0.009137  eta: 0h12m  tot: 0h3m2s  (19.2%)96.6%  lr: 0.040411  loss: 0.009122  eta: 0h12m  tot: 0h3m3s  (19.3%)96.7%  lr: 0.040401  loss: 0.009117  eta: 0h12m  tot: 0h3m3s  (19.3%)96.8%  lr: 0.040401  loss: 0.009110  eta: 0h12m  tot: 0h3m3s  (19.4%)97.2%  lr: 0.040361  loss: 0.009091  eta: 0h12m  tot: 0h3m4s  (19.4%)97.6%  lr: 0.040310  loss: 0.009076  eta: 0h12m  tot: 0h3m5s  (19.5%)98.4%  lr: 0.040220  loss: 0.009044  eta: 0h12m  tot: 0h3m6s  (19.7%)99.0%  lr: 0.040150  loss: 0.009023  eta: 0h12m  tot: 0h3m8s  (19.8%)\n",
      " ---+++                Epoch    0 Train error : 0.00895397 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65.2%  lr: 0.033894  loss: 0.002704  eta: 0h9m  tot: 0h5m4s  (33.0%)))002687  eta: 0h8m  tot: 0h3m12s  (20.0%)0.4%  lr: 0.039990  loss: 0.003654  eta: 0h11m  tot: 0h3m12s  (20.1%)1.7%  lr: 0.039920  loss: 0.002780  eta: 0h11m  tot: 0h3m14s  (20.3%)1.9%  lr: 0.039910  loss: 0.002780  eta: 0h11m  tot: 0h3m15s  (20.4%)2.4%  lr: 0.039880  loss: 0.002661  eta: 0h11m  tot: 0h3m16s  (20.5%)2.7%  lr: 0.039840  loss: 0.002824  eta: 0h11m  tot: 0h3m16s  (20.5%)3.1%  lr: 0.039770  loss: 0.002805  eta: 0h11m  tot: 0h3m17s  (20.6%)3.2%  lr: 0.039730  loss: 0.002826  eta: 0h11m  tot: 0h3m17s  (20.6%)4.4%  lr: 0.039560  loss: 0.002612  eta: 0h11m  tot: 0h3m19s  (20.9%)6.1%  lr: 0.039379  loss: 0.002716  eta: 0h11m  tot: 0h3m22s  (21.2%)6.6%  lr: 0.039349  loss: 0.002711  eta: 0h11m  tot: 0h3m23s  (21.3%)7.2%  lr: 0.039219  loss: 0.002690  eta: 0h11m  tot: 0h3m24s  (21.4%)7.3%  lr: 0.039199  loss: 0.002683  eta: 0h11m  tot: 0h3m24s  (21.5%)7.7%  lr: 0.039189  loss: 0.002704  eta: 0h11m  tot: 0h3m25s  (21.5%)8.3%  lr: 0.039139  loss: 0.002684  eta: 0h11m  tot: 0h3m26s  (21.7%)8.8%  lr: 0.039099  loss: 0.002667  eta: 0h11m  tot: 0h3m27s  (21.8%)11.0%  lr: 0.038959  loss: 0.002691  eta: 0h10m  tot: 0h3m30s  (22.2%)0.038839  loss: 0.002718  eta: 0h11m  tot: 0h3m31s  (22.3%)11.9%  lr: 0.038809  loss: 0.002735  eta: 0h11m  tot: 0h3m32s  (22.4%)12.9%  lr: 0.038729  loss: 0.002730  eta: 0h10m  tot: 0h3m33s  (22.6%)13.9%  lr: 0.038559  loss: 0.002729  eta: 0h11m  tot: 0h3m35s  (22.8%)14.0%  lr: 0.038549  loss: 0.002721  eta: 0h11m  tot: 0h3m35s  (22.8%)  tot: 0h3m36s  (22.8%)14.3%  lr: 0.038539  loss: 0.002707  eta: 0h11m  tot: 0h3m36s  (22.9%)14.5%  lr: 0.038519  loss: 0.002711  eta: 0h11m  tot: 0h3m36s  (22.9%)14.6%  lr: 0.038499  loss: 0.002719  eta: 0h11m  tot: 0h3m37s  (22.9%)15.0%  lr: 0.038438  loss: 0.002712  eta: 0h11m  tot: 0h3m37s  (23.0%)15.1%  lr: 0.038418  loss: 0.002704  eta: 0h11m  tot: 0h3m38s  (23.0%)15.9%  lr: 0.038328  loss: 0.002686  eta: 0h11m  tot: 0h3m39s  (23.2%)16.3%  lr: 0.038318  loss: 0.002669  eta: 0h10m  tot: 0h3m40s  (23.3%)17.1%  lr: 0.038238  loss: 0.002669  eta: 0h11m  tot: 0h3m41s  (23.4%)17.9%  lr: 0.038158  loss: 0.002666  eta: 0h11m  tot: 0h3m42s  (23.6%)18.0%  lr: 0.038148  loss: 0.002674  eta: 0h11m  tot: 0h3m43s  (23.6%)18.5%  lr: 0.038098  loss: 0.002677  eta: 0h11m  tot: 0h3m44s  (23.7%)19.1%  lr: 0.038048  loss: 0.002669  eta: 0h11m  tot: 0h3m45s  (23.8%)19.4%  lr: 0.037998  loss: 0.002672  eta: 0h11m  tot: 0h3m45s  (23.9%)20.2%  lr: 0.037878  loss: 0.002662  eta: 0h11m  tot: 0h3m47s  (24.0%)20.3%  lr: 0.037868  loss: 0.002667  eta: 0h10m  tot: 0h3m47s  (24.1%)0h3m48s  (24.1%)21.0%  lr: 0.037788  loss: 0.002687  eta: 0h11m  tot: 0h3m48s  (24.2%)21.2%  lr: 0.037778  loss: 0.002689  eta: 0h11m  tot: 0h3m48s  (24.2%)22.1%  lr: 0.037638  loss: 0.002681  eta: 0h11m  tot: 0h3m50s  (24.4%)22.5%  lr: 0.037578  loss: 0.002665  eta: 0h10m  tot: 0h3m51s  (24.5%)3m52s  (24.6%)52s  (24.6%)  eta: 0h10m  tot: 0h3m53s  (24.8%)23.9%  lr: 0.037417  loss: 0.002692  eta: 0h10m  tot: 0h3m53s  (24.8%)24.2%  lr: 0.037387  loss: 0.002699  eta: 0h10m  tot: 0h3m54s  (24.8%)24.3%  lr: 0.037367  loss: 0.002703  eta: 0h10m  tot: 0h3m54s  (24.9%)24.9%  lr: 0.037317  loss: 0.002702  eta: 0h10m  tot: 0h3m55s  (25.0%)25.2%  lr: 0.037317  loss: 0.002698  eta: 0h10m  tot: 0h3m56s  (25.0%)26.5%  lr: 0.037177  loss: 0.002742  eta: 0h10m  tot: 0h3m58s  (25.3%)27.0%  lr: 0.037117  loss: 0.002748  eta: 0h10m  tot: 0h3m59s  (25.4%)27.1%  lr: 0.037117  loss: 0.002747  eta: 0h10m  tot: 0h3m59s  (25.4%)27.5%  lr: 0.037057  loss: 0.002747  eta: 0h10m  tot: 0h4m0s  (25.5%)27.9%  lr: 0.037037  loss: 0.002749  eta: 0h10m  tot: 0h4m0s  (25.6%)28.0%  lr: 0.037037  loss: 0.002748  eta: 0h10m  tot: 0h4m0s  (25.6%)28.5%  lr: 0.037007  loss: 0.002747  eta: 0h10m  tot: 0h4m1s  (25.7%)28.6%  lr: 0.036997  loss: 0.002742  eta: 0h10m  tot: 0h4m2s  (25.7%)28.8%  lr: 0.036967  loss: 0.002741  eta: 0h10m  tot: 0h4m2s  (25.8%)30.8%  lr: 0.036787  loss: 0.002732  eta: 0h10m  tot: 0h4m5s  (26.2%)31.1%  lr: 0.036777  loss: 0.002736  eta: 0h10m  tot: 0h4m6s  (26.2%)31.3%  lr: 0.036767  loss: 0.002733  eta: 0h10m  tot: 0h4m6s  (26.3%)31.6%  lr: 0.036737  loss: 0.002732  eta: 0h10m  tot: 0h4m6s  (26.3%)33.0%  lr: 0.036617  loss: 0.002717  eta: 0h10m  tot: 0h4m9s  (26.6%)33.8%  lr: 0.036577  loss: 0.002699  eta: 0h10m  tot: 0h4m10s  (26.8%)34.6%  lr: 0.036477  loss: 0.002696  eta: 0h10m  tot: 0h4m11s  (26.9%)35.1%  lr: 0.036406  loss: 0.002689  eta: 0h10m  tot: 0h4m12s  (27.0%)35.2%  lr: 0.036396  loss: 0.002690  eta: 0h10m  tot: 0h4m12s  (27.0%)35.6%  lr: 0.036376  loss: 0.002693  eta: 0h10m  tot: 0h4m13s  (27.1%)35.7%  lr: 0.036376  loss: 0.002688  eta: 0h10m  tot: 0h4m13s  (27.1%)36.4%  lr: 0.036296  loss: 0.002689  eta: 0h10m  tot: 0h4m14s  (27.3%)36.7%  lr: 0.036276  loss: 0.002685  eta: 0h10m  tot: 0h4m15s  (27.3%)37.3%  lr: 0.036196  loss: 0.002678  eta: 0h10m  tot: 0h4m15s  (27.5%)38.2%  lr: 0.036126  loss: 0.002682  eta: 0h10m  tot: 0h4m17s  (27.6%)38.3%  lr: 0.036126  loss: 0.002678  eta: 0h10m  tot: 0h4m17s  (27.7%)39.2%  lr: 0.036056  loss: 0.002664  eta: 0h10m  tot: 0h4m18s  (27.8%)39.4%  lr: 0.036056  loss: 0.002663  eta: 0h10m  tot: 0h4m19s  (27.9%)39.9%  lr: 0.036026  loss: 0.002653  eta: 0h10m  tot: 0h4m19s  (28.0%)40.0%  lr: 0.036026  loss: 0.002652  eta: 0h10m  tot: 0h4m20s  (28.0%)40.1%  lr: 0.036026  loss: 0.002649  eta: 0h10m  tot: 0h4m20s  (28.0%)40.6%  lr: 0.035996  loss: 0.002644  eta: 0h10m  tot: 0h4m21s  (28.1%)41.8%  lr: 0.035936  loss: 0.002641  eta: 0h10m  tot: 0h4m23s  (28.4%)42.5%  lr: 0.035866  loss: 0.002645  eta: 0h10m  tot: 0h4m24s  (28.5%)42.8%  lr: 0.035846  loss: 0.002648  eta: 0h10m  tot: 0h4m25s  (28.6%)43.0%  lr: 0.035836  loss: 0.002651  eta: 0h10m  tot: 0h4m25s  (28.6%)43.6%  lr: 0.035766  loss: 0.002653  eta: 0h10m  tot: 0h4m26s  (28.7%)46.5%  lr: 0.035566  loss: 0.002658  eta: 0h10m  tot: 0h4m31s  (29.3%)48.5%  lr: 0.035385  loss: 0.002650  eta: 0h9m  tot: 0h4m34s  (29.7%)49.3%  lr: 0.035345  loss: 0.002656  eta: 0h9m  tot: 0h4m35s  (29.9%)49.7%  lr: 0.035295  loss: 0.002658  eta: 0h9m  tot: 0h4m36s  (29.9%)%  lr: 0.035215  loss: 0.002647  eta: 0h9m  tot: 0h4m37s  (30.1%)51.0%  lr: 0.035165  loss: 0.002657  eta: 0h9m  tot: 0h4m38s  (30.2%)51.1%  lr: 0.035165  loss: 0.002661  eta: 0h9m  tot: 0h4m38s  (30.2%)%  lr: 0.035145  loss: 0.002666  eta: 0h9m  tot: 0h4m39s  (30.3%)51.9%  lr: 0.035125  loss: 0.002670  eta: 0h9m  tot: 0h4m39s  (30.4%)52.2%  lr: 0.035085  loss: 0.002673  eta: 0h9m  tot: 0h4m40s  (30.4%)52.4%  lr: 0.035085  loss: 0.002673  eta: 0h9m  tot: 0h4m40s  (30.5%)52.6%  lr: 0.035055  loss: 0.002680  eta: 0h9m  tot: 0h4m41s  (30.5%)52.7%  lr: 0.035045  loss: 0.002681  eta: 0h9m  tot: 0h4m41s  (30.5%)53.3%  lr: 0.034995  loss: 0.002680  eta: 0h9m  tot: 0h4m42s  (30.7%)53.9%  lr: 0.034925  loss: 0.002687  eta: 0h9m  tot: 0h4m43s  (30.8%)54.0%  lr: 0.034905  loss: 0.002687  eta: 0h9m  tot: 0h4m43s  (30.8%)54.7%  lr: 0.034845  loss: 0.002682  eta: 0h9m  tot: 0h4m44s  (30.9%)54.8%  lr: 0.034845  loss: 0.002686  eta: 0h9m  tot: 0h4m45s  (31.0%)55.5%  lr: 0.034735  loss: 0.002678  eta: 0h9m  tot: 0h4m46s  (31.1%)%  lr: 0.034725  loss: 0.002675  eta: 0h9m  tot: 0h4m46s  (31.1%)55.7%  lr: 0.034725  loss: 0.002676  eta: 0h9m  tot: 0h4m46s  (31.1%)55.9%  lr: 0.034675  loss: 0.002673  eta: 0h9m  tot: 0h4m46s  (31.2%)56.3%  lr: 0.034655  loss: 0.002673  eta: 0h9m  tot: 0h4m47s  (31.3%)56.8%  lr: 0.034615  loss: 0.002670  eta: 0h9m  tot: 0h4m48s  (31.4%)57.2%  lr: 0.034585  loss: 0.002676  eta: 0h9m  tot: 0h4m49s  (31.4%)57.3%  lr: 0.034585  loss: 0.002674  eta: 0h9m  tot: 0h4m49s  (31.5%)59.3%  lr: 0.034435  loss: 0.002675  eta: 0h9m  tot: 0h4m52s  (31.9%)59.7%  lr: 0.034404  loss: 0.002674  eta: 0h9m  tot: 0h4m53s  (31.9%)%  lr: 0.034224  loss: 0.002691  eta: 0h9m  tot: 0h4m57s  (32.4%)64.2%  lr: 0.033994  loss: 0.002699  eta: 0h9m  tot: 0h5m2s  (32.8%)64.3%  lr: 0.033974  loss: 0.002699  eta: 0h9m  tot: 0h5m2s  (32.9%)65.0%  lr: 0.033904  loss: 0.002703  eta: 0h9m  tot: 0h5m4s  (33.0%)65.3%  lr: 0.033894  loss: 0.002703  eta: 0h9m  tot: 0h5m4s  (33.1%)65.4%  lr: 0.033894  loss: 0.002704  eta: 0h9m  tot: 0h5m4s  (33.1%)65.4%  lr: 0.033884  loss: 0.002701  eta: 0h9m  tot: 0h5m5s  (33.1%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030310  loss: 0.002731  eta: 0h8m  tot: 0h6m4s  (40.0%)5.8%  lr: 0.033794  loss: 0.002703  eta: 0h9m  tot: 0h5m5s  (33.2%)66.6%  lr: 0.033744  loss: 0.002700  eta: 0h9m  tot: 0h5m6s  (33.3%)66.7%  lr: 0.033734  loss: 0.002701  eta: 0h9m  tot: 0h5m7s  (33.3%)67.2%  lr: 0.033724  loss: 0.002703  eta: 0h9m  tot: 0h5m8s  (33.4%)67.7%  lr: 0.033694  loss: 0.002701  eta: 0h9m  tot: 0h5m8s  (33.5%)68.5%  lr: 0.033624  loss: 0.002705  eta: 0h9m  tot: 0h5m10s  (33.7%)68.9%  lr: 0.033554  loss: 0.002702  eta: 0h9m  tot: 0h5m10s  (33.8%)  lr: 0.033454  loss: 0.002703  eta: 0h9m  tot: 0h5m12s  (34.0%)71.2%  lr: 0.033333  loss: 0.002700  eta: 0h9m  tot: 0h5m15s  (34.2%)71.7%  lr: 0.033283  loss: 0.002698  eta: 0h9m  tot: 0h5m15s  (34.3%)72.4%  lr: 0.033243  loss: 0.002702  eta: 0h9m  tot: 0h5m17s  (34.5%)72.6%  lr: 0.033223  loss: 0.002699  eta: 0h9m  tot: 0h5m17s  (34.5%)73.1%  lr: 0.033183  loss: 0.002701  eta: 0h9m  tot: 0h5m18s  (34.6%)73.9%  lr: 0.033103  loss: 0.002710  eta: 0h9m  tot: 0h5m19s  (34.8%)74.1%  lr: 0.033093  loss: 0.002711  eta: 0h9m  tot: 0h5m20s  (34.8%)74.2%  lr: 0.033083  loss: 0.002710  eta: 0h9m  tot: 0h5m20s  (34.8%)75.5%  lr: 0.033013  loss: 0.002712  eta: 0h9m  tot: 0h5m22s  (35.1%)76.3%  lr: 0.032903  loss: 0.002721  eta: 0h9m  tot: 0h5m23s  (35.3%)77.7%  lr: 0.032753  loss: 0.002736  eta: 0h9m  tot: 0h5m27s  (35.5%)77.9%  lr: 0.032713  loss: 0.002734  eta: 0h9m  tot: 0h5m27s  (35.6%)78.3%  lr: 0.032643  loss: 0.002729  eta: 0h9m  tot: 0h5m28s  (35.7%)78.8%  lr: 0.032583  loss: 0.002728  eta: 0h9m  tot: 0h5m29s  (35.8%)78.9%  lr: 0.032553  loss: 0.002732  eta: 0h9m  tot: 0h5m29s  (35.8%)79.3%  lr: 0.032523  loss: 0.002733  eta: 0h9m  tot: 0h5m29s  (35.9%)79.4%  lr: 0.032523  loss: 0.002734  eta: 0h9m  tot: 0h5m30s  (35.9%)80.2%  lr: 0.032483  loss: 0.002737  eta: 0h9m  tot: 0h5m31s  (36.0%)%  lr: 0.032393  loss: 0.002740  eta: 0h9m  tot: 0h5m32s  (36.2%)81.2%  lr: 0.032362  loss: 0.002742  eta: 0h9m  tot: 0h5m32s  (36.2%)81.7%  lr: 0.032332  loss: 0.002743  eta: 0h9m  tot: 0h5m33s  (36.3%)%  lr: 0.032172  loss: 0.002744  eta: 0h9m  tot: 0h5m35s  (36.6%)85.3%  lr: 0.031972  loss: 0.002744  eta: 0h9m  tot: 0h5m38s  (37.1%)85.6%  lr: 0.031922  loss: 0.002748  eta: 0h9m  tot: 0h5m39s  (37.1%)88.2%  lr: 0.031642  loss: 0.002757  eta: 0h8m  tot: 0h5m43s  (37.6%)88.6%  lr: 0.031582  loss: 0.002753  eta: 0h8m  tot: 0h5m44s  (37.7%)89.0%  lr: 0.031532  loss: 0.002747  eta: 0h8m  tot: 0h5m45s  (37.8%)90.2%  lr: 0.031382  loss: 0.002742  eta: 0h8m  tot: 0h5m47s  (38.0%)92.2%  lr: 0.031111  loss: 0.002738  eta: 0h8m  tot: 0h5m51s  (38.4%)93.4%  lr: 0.030991  loss: 0.002736  eta: 0h8m  tot: 0h5m53s  (38.7%)93.7%  lr: 0.030971  loss: 0.002738  eta: 0h8m  tot: 0h5m53s  (38.7%)94.3%  lr: 0.030941  loss: 0.002737  eta: 0h8m  tot: 0h5m54s  (38.9%)94.6%  lr: 0.030911  loss: 0.002740  eta: 0h8m  tot: 0h5m55s  (38.9%)95.2%  lr: 0.030881  loss: 0.002740  eta: 0h8m  tot: 0h5m56s  (39.0%)95.6%  lr: 0.030831  loss: 0.002740  eta: 0h8m  tot: 0h5m56s  (39.1%)97.1%  lr: 0.030611  loss: 0.002731  eta: 0h8m  tot: 0h5m59s  (39.4%)97.3%  lr: 0.030591  loss: 0.002733  eta: 0h8m  tot: 0h5m59s  (39.5%)97.4%  lr: 0.030571  loss: 0.002738  eta: 0h8m  tot: 0h6m0s  (39.5%)97.9%  lr: 0.030561  loss: 0.002737  eta: 0h8m  tot: 0h6m0s  (39.6%)98.1%  lr: 0.030521  loss: 0.002737  eta: 0h8m  tot: 0h6m1s  (39.6%)98.6%  lr: 0.030491  loss: 0.002736  eta: 0h8m  tot: 0h6m2s  (39.7%)98.8%  lr: 0.030471  loss: 0.002736  eta: 0h8m  tot: 0h6m2s  (39.8%)99.4%  lr: 0.030371  loss: 0.002734  eta: 0h8m  tot: 0h6m3s  (39.9%)\n",
      " ---+++                Epoch    1 Train error : 0.00268672 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62.2%  lr: 0.023944  loss: 0.001899  eta: 0h6m  tot: 0h7m56s  (52.4%)4%  lr: 0.029980  loss: 0.001674  eta: 0h7m  tot: 0h6m9s  (40.1%)1.3%  lr: 0.029920  loss: 0.001571  eta: 0h7m  tot: 0h6m11s  (40.3%)%  lr: 0.029880  loss: 0.001647  eta: 0h8m  tot: 0h6m11s  (40.3%)1.8%  lr: 0.029880  loss: 0.001681  eta: 0h8m  tot: 0h6m12s  (40.4%)h6m13s  (40.6%)3.0%  lr: 0.029780  loss: 0.001766  eta: 0h8m  tot: 0h6m14s  (40.6%)3.1%  lr: 0.029780  loss: 0.001770  eta: 0h8m  tot: 0h6m14s  (40.6%)  lr: 0.029720  loss: 0.001752  eta: 0h8m  tot: 0h6m15s  (40.8%)5.0%  lr: 0.029570  loss: 0.001760  eta: 0h8m  tot: 0h6m17s  (41.0%)6.0%  lr: 0.029439  loss: 0.001746  eta: 0h8m  tot: 0h6m19s  (41.2%)6.4%  lr: 0.029409  loss: 0.001812  eta: 0h8m  tot: 0h6m20s  (41.3%)6.7%  lr: 0.029399  loss: 0.001802  eta: 0h8m  tot: 0h6m20s  (41.3%)8.2%  lr: 0.029199  loss: 0.001783  eta: 0h8m  tot: 0h6m23s  (41.6%)8.6%  lr: 0.029169  loss: 0.001812  eta: 0h8m  tot: 0h6m24s  (41.7%)8.8%  lr: 0.029149  loss: 0.001803  eta: 0h8m  tot: 0h6m24s  (41.8%)8.9%  lr: 0.029139  loss: 0.001803  eta: 0h8m  tot: 0h6m25s  (41.8%)9.0%  lr: 0.029139  loss: 0.001829  eta: 0h8m  tot: 0h6m25s  (41.8%)9.1%  lr: 0.029109  loss: 0.001817  eta: 0h8m  tot: 0h6m25s  (41.8%)9.2%  lr: 0.029109  loss: 0.001811  eta: 0h8m  tot: 0h6m25s  (41.8%)9.6%  lr: 0.029049  loss: 0.001833  eta: 0h8m  tot: 0h6m26s  (41.9%)9.7%  lr: 0.029029  loss: 0.001853  eta: 0h8m  tot: 0h6m26s  (41.9%)9.9%  lr: 0.029009  loss: 0.001858  eta: 0h8m  tot: 0h6m26s  (42.0%)10.4%  lr: 0.028979  loss: 0.001853  eta: 0h8m  tot: 0h6m27s  (42.1%)10.8%  lr: 0.028929  loss: 0.001865  eta: 0h8m  tot: 0h6m28s  (42.2%)%  lr: 0.028909  loss: 0.001850  eta: 0h8m  tot: 0h6m28s  (42.2%)11.9%  lr: 0.028839  loss: 0.001830  eta: 0h8m  tot: 0h6m30s  (42.4%)12.5%  lr: 0.028819  loss: 0.001834  eta: 0h8m  tot: 0h6m31s  (42.5%)12.6%  lr: 0.028819  loss: 0.001837  eta: 0h8m  tot: 0h6m31s  (42.5%)13.1%  lr: 0.028789  loss: 0.001837  eta: 0h8m  tot: 0h6m32s  (42.6%)13.2%  lr: 0.028789  loss: 0.001831  eta: 0h8m  tot: 0h6m32s  (42.6%)13.6%  lr: 0.028769  loss: 0.001823  eta: 0h8m  tot: 0h6m32s  (42.7%)15.4%  lr: 0.028599  loss: 0.001840  eta: 0h8m  tot: 0h6m35s  (43.1%)15.7%  lr: 0.028589  loss: 0.001841  eta: 0h8m  tot: 0h6m36s  (43.1%)16.7%  lr: 0.028519  loss: 0.001849  eta: 0h8m  tot: 0h6m38s  (43.3%)0h6m38s  (43.4%)17.3%  lr: 0.028438  loss: 0.001852  eta: 0h8m  tot: 0h6m38s  (43.5%)18.5%  lr: 0.028348  loss: 0.001848  eta: 0h8m  tot: 0h6m40s  (43.7%)%  lr: 0.028318  loss: 0.001862  eta: 0h8m  tot: 0h6m41s  (43.8%)19.2%  lr: 0.028248  loss: 0.001858  eta: 0h8m  tot: 0h6m42s  (43.8%)19.6%  lr: 0.028218  loss: 0.001843  eta: 0h8m  tot: 0h6m43s  (43.9%)20.4%  lr: 0.028138  loss: 0.001857  eta: 0h8m  tot: 0h6m44s  (44.1%)21.3%  lr: 0.028038  loss: 0.001847  eta: 0h7m  tot: 0h6m45s  (44.3%)21.4%  lr: 0.028028  loss: 0.001846  eta: 0h7m  tot: 0h6m45s  (44.3%)21.8%  lr: 0.027938  loss: 0.001854  eta: 0h7m  tot: 0h6m46s  (44.4%)21.9%  lr: 0.027938  loss: 0.001858  eta: 0h7m  tot: 0h6m46s  (44.4%)22.4%  lr: 0.027898  loss: 0.001859  eta: 0h7m  tot: 0h6m47s  (44.5%)22.7%  lr: 0.027868  loss: 0.001855  eta: 0h7m  tot: 0h6m47s  (44.5%)22.9%  lr: 0.027858  loss: 0.001847  eta: 0h7m  tot: 0h6m48s  (44.6%)24.2%  lr: 0.027758  loss: 0.001868  eta: 0h7m  tot: 0h6m50s  (44.8%)24.5%  lr: 0.027718  loss: 0.001880  eta: 0h7m  tot: 0h6m50s  (44.9%)24.7%  lr: 0.027688  loss: 0.001880  eta: 0h7m  tot: 0h6m51s  (44.9%)24.8%  lr: 0.027668  loss: 0.001877  eta: 0h7m  tot: 0h6m51s  (45.0%)  lr: 0.027628  loss: 0.001877  eta: 0h7m  tot: 0h6m52s  (45.0%)%  lr: 0.027628  loss: 0.001868  eta: 0h7m  tot: 0h6m52s  (45.1%)25.7%  lr: 0.027628  loss: 0.001865  eta: 0h7m  tot: 0h6m52s  (45.1%)%  lr: 0.027548  loss: 0.001869  eta: 0h7m  tot: 0h6m53s  (45.2%)26.5%  lr: 0.027498  loss: 0.001865  eta: 0h7m  tot: 0h6m54s  (45.3%)27.8%  lr: 0.027337  loss: 0.001864  eta: 0h7m  tot: 0h6m56s  (45.6%)28.3%  lr: 0.027287  loss: 0.001860  eta: 0h7m  tot: 0h6m57s  (45.7%)28.5%  lr: 0.027277  loss: 0.001862  eta: 0h7m  tot: 0h6m57s  (45.7%)%  lr: 0.027117  loss: 0.001852  eta: 0h7m  tot: 0h6m59s  (45.9%)29.8%  lr: 0.027107  loss: 0.001850  eta: 0h7m  tot: 0h6m59s  (46.0%)30.2%  lr: 0.027097  loss: 0.001852  eta: 0h7m  tot: 0h7m0s  (46.0%)30.9%  lr: 0.027037  loss: 0.001862  eta: 0h7m  tot: 0h7m1s  (46.2%)31.2%  lr: 0.027017  loss: 0.001855  eta: 0h7m  tot: 0h7m1s  (46.2%)31.6%  lr: 0.026977  loss: 0.001861  eta: 0h7m  tot: 0h7m2s  (46.3%)31.7%  lr: 0.026977  loss: 0.001861  eta: 0h7m  tot: 0h7m2s  (46.3%)32.9%  lr: 0.026867  loss: 0.001849  eta: 0h7m  tot: 0h7m4s  (46.6%)33.8%  lr: 0.026747  loss: 0.001853  eta: 0h7m  tot: 0h7m6s  (46.8%)34.6%  lr: 0.026647  loss: 0.001845  eta: 0h7m  tot: 0h7m7s  (46.9%)35.4%  lr: 0.026607  loss: 0.001859  eta: 0h7m  tot: 0h7m9s  (47.1%)36.2%  lr: 0.026497  loss: 0.001878  eta: 0h7m  tot: 0h7m10s  (47.2%)36.7%  lr: 0.026426  loss: 0.001876  eta: 0h7m  tot: 0h7m11s  (47.3%)%  lr: 0.026406  loss: 0.001876  eta: 0h7m  tot: 0h7m11s  (47.4%)38.0%  lr: 0.026256  loss: 0.001883  eta: 0h7m  tot: 0h7m13s  (47.6%)39.1%  lr: 0.026176  loss: 0.001876  eta: 0h7m  tot: 0h7m15s  (47.8%)h7m15s  (47.8%)39.5%  lr: 0.026146  loss: 0.001871  eta: 0h7m  tot: 0h7m16s  (47.9%)39.6%  lr: 0.026106  loss: 0.001871  eta: 0h7m  tot: 0h7m16s  (47.9%)40.1%  lr: 0.026056  loss: 0.001876  eta: 0h7m  tot: 0h7m17s  (48.0%)41.2%  lr: 0.025886  loss: 0.001885  eta: 0h7m  tot: 0h7m19s  (48.2%)41.7%  lr: 0.025856  loss: 0.001889  eta: 0h7m  tot: 0h7m20s  (48.3%)42.0%  lr: 0.025786  loss: 0.001887  eta: 0h7m  tot: 0h7m20s  (48.4%)42.2%  lr: 0.025766  loss: 0.001891  eta: 0h7m  tot: 0h7m21s  (48.4%)42.4%  lr: 0.025726  loss: 0.001892  eta: 0h7m  tot: 0h7m21s  (48.5%)42.8%  lr: 0.025696  loss: 0.001893  eta: 0h7m  tot: 0h7m22s  (48.6%)42.9%  lr: 0.025686  loss: 0.001892  eta: 0h7m  tot: 0h7m22s  (48.6%)43.2%  lr: 0.025666  loss: 0.001890  eta: 0h7m  tot: 0h7m22s  (48.6%)43.6%  lr: 0.025606  loss: 0.001889  eta: 0h7m  tot: 0h7m23s  (48.7%)43.8%  lr: 0.025576  loss: 0.001887  eta: 0h7m  tot: 0h7m24s  (48.8%)43.9%  lr: 0.025576  loss: 0.001888  eta: 0h7m  tot: 0h7m24s  (48.8%)45.3%  lr: 0.025496  loss: 0.001879  eta: 0h7m  tot: 0h7m26s  (49.1%)45.5%  lr: 0.025486  loss: 0.001881  eta: 0h7m  tot: 0h7m26s  (49.1%)46.0%  lr: 0.025436  loss: 0.001874  eta: 0h7m  tot: 0h7m27s  (49.2%)48.2%  lr: 0.025235  loss: 0.001878  eta: 0h7m  tot: 0h7m31s  (49.6%)48.4%  lr: 0.025215  loss: 0.001883  eta: 0h7m  tot: 0h7m31s  (49.7%)49.1%  lr: 0.025155  loss: 0.001881  eta: 0h7m  tot: 0h7m32s  (49.8%)49.2%  lr: 0.025125  loss: 0.001882  eta: 0h7m  tot: 0h7m33s  (49.8%)49.7%  lr: 0.025095  loss: 0.001878  eta: 0h7m  tot: 0h7m33s  (49.9%)51.9%  lr: 0.024895  loss: 0.001874  eta: 0h7m  tot: 0h7m37s  (50.4%)52.1%  lr: 0.024865  loss: 0.001875  eta: 0h7m  tot: 0h7m38s  (50.4%)52.3%  lr: 0.024855  loss: 0.001878  eta: 0h7m  tot: 0h7m38s  (50.5%)52.5%  lr: 0.024845  loss: 0.001881  eta: 0h7m  tot: 0h7m39s  (50.5%)53.0%  lr: 0.024815  loss: 0.001878  eta: 0h7m  tot: 0h7m39s  (50.6%)53.4%  lr: 0.024795  loss: 0.001877  eta: 0h7m  tot: 0h7m40s  (50.7%)54.9%  lr: 0.024675  loss: 0.001881  eta: 0h6m  tot: 0h7m43s  (51.0%)55.0%  lr: 0.024655  loss: 0.001882  eta: 0h6m  tot: 0h7m43s  (51.0%)55.5%  lr: 0.024615  loss: 0.001878  eta: 0h6m  tot: 0h7m44s  (51.1%)56.2%  lr: 0.024515  loss: 0.001879  eta: 0h6m  tot: 0h7m45s  (51.2%)56.7%  lr: 0.024425  loss: 0.001877  eta: 0h6m  tot: 0h7m46s  (51.3%)57.1%  lr: 0.024374  loss: 0.001876  eta: 0h6m  tot: 0h7m47s  (51.4%)%  lr: 0.024344  loss: 0.001876  eta: 0h6m  tot: 0h7m47s  (51.5%)57.3%  lr: 0.024344  loss: 0.001878  eta: 0h6m  tot: 0h7m48s  (51.5%)58.7%  lr: 0.024244  loss: 0.001890  eta: 0h6m  tot: 0h7m50s  (51.7%)59.1%  lr: 0.024224  loss: 0.001886  eta: 0h6m  tot: 0h7m50s  (51.8%)59.5%  lr: 0.024174  loss: 0.001889  eta: 0h6m  tot: 0h7m51s  (51.9%)h6m  tot: 0h7m51s  (51.9%)60.3%  lr: 0.024084  loss: 0.001898  eta: 0h6m  tot: 0h7m53s  (52.1%)60.9%  lr: 0.024034  loss: 0.001901  eta: 0h6m  tot: 0h7m54s  (52.2%)61.0%  lr: 0.024034  loss: 0.001902  eta: 0h6m  tot: 0h7m54s  (52.2%)61.9%  lr: 0.023954  loss: 0.001899  eta: 0h6m  tot: 0h7m55s  (52.4%)62.3%  lr: 0.023944  loss: 0.001897  eta: 0h6m  tot: 0h7m56s  (52.5%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0  lr: 0.020100  loss: 0.001875  eta: 0h5m  tot: 0h8m59s  (60.0%)62.7%  lr: 0.023864  loss: 0.001898  eta: 0h6m  tot: 0h7m57s  (52.5%)63.3%  lr: 0.023804  loss: 0.001893  eta: 0h6m  tot: 0h7m58s  (52.7%)63.6%  lr: 0.023774  loss: 0.001893  eta: 0h6m  tot: 0h7m58s  (52.7%)64.0%  lr: 0.023744  loss: 0.001892  eta: 0h6m  tot: 0h7m59s  (52.8%)65.7%  lr: 0.023584  loss: 0.001891  eta: 0h6m  tot: 0h8m2s  (53.1%)66.0%  lr: 0.023554  loss: 0.001889  eta: 0h6m  tot: 0h8m2s  (53.2%)66.3%  lr: 0.023524  loss: 0.001885  eta: 0h6m  tot: 0h8m3s  (53.3%)67.2%  lr: 0.023434  loss: 0.001886  eta: 0h6m  tot: 0h8m4s  (53.4%)67.6%  lr: 0.023404  loss: 0.001888  eta: 0h6m  tot: 0h8m5s  (53.5%)68.2%  lr: 0.023343  loss: 0.001892  eta: 0h6m  tot: 0h8m6s  (53.6%)68.3%  lr: 0.023343  loss: 0.001893  eta: 0h6m  tot: 0h8m6s  (53.7%)68.5%  lr: 0.023343  loss: 0.001894  eta: 0h6m  tot: 0h8m6s  (53.7%)69.0%  lr: 0.023303  loss: 0.001897  eta: 0h6m  tot: 0h8m7s  (53.8%)69.3%  lr: 0.023233  loss: 0.001895  eta: 0h6m  tot: 0h8m8s  (53.9%)71.4%  lr: 0.022993  loss: 0.001894  eta: 0h6m  tot: 0h8m11s  (54.3%)71.7%  lr: 0.022943  loss: 0.001896  eta: 0h6m  tot: 0h8m12s  (54.3%)71.9%  lr: 0.022933  loss: 0.001896  eta: 0h6m  tot: 0h8m12s  (54.4%)72.5%  lr: 0.022903  loss: 0.001896  eta: 0h6m  tot: 0h8m13s  (54.5%)73.3%  lr: 0.022803  loss: 0.001897  eta: 0h6m  tot: 0h8m14s  (54.7%)73.4%  lr: 0.022773  loss: 0.001896  eta: 0h6m  tot: 0h8m15s  (54.7%)73.5%  lr: 0.022773  loss: 0.001895  eta: 0h6m  tot: 0h8m15s  (54.7%)0.022653  loss: 0.001889  eta: 0h6m  tot: 0h8m16s  (54.9%)%  lr: 0.022623  loss: 0.001889  eta: 0h6m  tot: 0h8m16s  (54.9%)75.0%  lr: 0.022583  loss: 0.001887  eta: 0h6m  tot: 0h8m17s  (55.0%)75.2%  lr: 0.022553  loss: 0.001886  eta: 0h6m  tot: 0h8m17s  (55.0%)76.2%  lr: 0.022473  loss: 0.001880  eta: 0h6m  tot: 0h8m19s  (55.2%)76.4%  lr: 0.022473  loss: 0.001878  eta: 0h6m  tot: 0h8m19s  (55.3%)77.6%  lr: 0.022372  loss: 0.001880  eta: 0h6m  tot: 0h8m21s  (55.5%)78.2%  lr: 0.022322  loss: 0.001878  eta: 0h6m  tot: 0h8m22s  (55.6%)78.3%  lr: 0.022322  loss: 0.001879  eta: 0h6m  tot: 0h8m22s  (55.7%)78.9%  lr: 0.022262  loss: 0.001879  eta: 0h6m  tot: 0h8m23s  (55.8%)79.0%  lr: 0.022262  loss: 0.001879  eta: 0h6m  tot: 0h8m23s  (55.8%)%  lr: 0.022022  loss: 0.001879  eta: 0h6m  tot: 0h8m27s  (56.2%)81.6%  lr: 0.021922  loss: 0.001879  eta: 0h6m  tot: 0h8m27s  (56.3%)81.7%  lr: 0.021912  loss: 0.001878  eta: 0h6m  tot: 0h8m28s  (56.3%)82.8%  lr: 0.021812  loss: 0.001881  eta: 0h6m  tot: 0h8m29s  (56.6%)83.5%  lr: 0.021742  loss: 0.001878  eta: 0h6m  tot: 0h8m30s  (56.7%)84.4%  lr: 0.021682  loss: 0.001875  eta: 0h6m  tot: 0h8m32s  (56.9%)85.8%  lr: 0.021512  loss: 0.001873  eta: 0h6m  tot: 0h8m34s  (57.2%)86.3%  lr: 0.021432  loss: 0.001871  eta: 0h6m  tot: 0h8m35s  (57.3%)86.4%  lr: 0.021432  loss: 0.001870  eta: 0h6m  tot: 0h8m35s  (57.3%)87.2%  lr: 0.021321  loss: 0.001873  eta: 0h6m  tot: 0h8m37s  (57.4%)87.9%  lr: 0.021261  loss: 0.001876  eta: 0h6m  tot: 0h8m38s  (57.6%)88.4%  lr: 0.021211  loss: 0.001872  eta: 0h6m  tot: 0h8m39s  (57.7%)89.1%  lr: 0.021191  loss: 0.001873  eta: 0h5m  tot: 0h8m41s  (57.8%)89.8%  lr: 0.021131  loss: 0.001873  eta: 0h5m  tot: 0h8m42s  (58.0%)90.2%  lr: 0.021071  loss: 0.001878  eta: 0h5m  tot: 0h8m42s  (58.0%)90.5%  lr: 0.021031  loss: 0.001880  eta: 0h5m  tot: 0h8m43s  (58.1%)90.7%  lr: 0.021011  loss: 0.001879  eta: 0h5m  tot: 0h8m43s  (58.1%)91.6%  lr: 0.020931  loss: 0.001877  eta: 0h5m  tot: 0h8m45s  (58.3%)92.2%  lr: 0.020901  loss: 0.001880  eta: 0h5m  tot: 0h8m46s  (58.4%)93.1%  lr: 0.020821  loss: 0.001875  eta: 0h5m  tot: 0h8m47s  (58.6%)93.3%  lr: 0.020771  loss: 0.001873  eta: 0h5m  tot: 0h8m48s  (58.7%)93.4%  lr: 0.020771  loss: 0.001874  eta: 0h5m  tot: 0h8m48s  (58.7%)93.4%  lr: 0.020761  loss: 0.001874  eta: 0h5m  tot: 0h8m48s  (58.7%)93.8%  lr: 0.020751  loss: 0.001875  eta: 0h5m  tot: 0h8m49s  (58.8%)95.0%  lr: 0.020631  loss: 0.001880  eta: 0h5m  tot: 0h8m50s  (59.0%)95.4%  lr: 0.020591  loss: 0.001881  eta: 0h5m  tot: 0h8m51s  (59.1%)96.1%  lr: 0.020511  loss: 0.001880  eta: 0h5m  tot: 0h8m52s  (59.2%)96.2%  lr: 0.020501  loss: 0.001880  eta: 0h5m  tot: 0h8m52s  (59.2%)96.4%  lr: 0.020501  loss: 0.001878  eta: 0h5m  tot: 0h8m53s  (59.3%)96.8%  lr: 0.020441  loss: 0.001876  eta: 0h5m  tot: 0h8m53s  (59.4%)%  lr: 0.020411  loss: 0.001877  eta: 0h5m  tot: 0h8m54s  (59.4%)97.4%  lr: 0.020340  loss: 0.001875  eta: 0h5m  tot: 0h8m55s  (59.5%)97.5%  lr: 0.020320  loss: 0.001876  eta: 0h5m  tot: 0h8m55s  (59.5%)98.5%  lr: 0.020220  loss: 0.001872  eta: 0h5m  tot: 0h8m57s  (59.7%)98.7%  lr: 0.020220  loss: 0.001871  eta: 0h5m  tot: 0h8m57s  (59.7%)98.8%  lr: 0.020190  loss: 0.001870  eta: 0h5m  tot: 0h8m57s  (59.8%)99.4%  lr: 0.020180  loss: 0.001874  eta: 0h5m  tot: 0h8m58s  (59.9%)99.6%  lr: 0.020150  loss: 0.001874  eta: 0h5m  tot: 0h8m58s  (59.9%)99.9%  lr: 0.020100  loss: 0.001874  eta: 0h5m  tot: 0h8m59s  (60.0%)%  lr: 0.020090  loss: 0.001875  eta: 0h5m  tot: 0h8m59s  (60.0%)\n",
      " ---+++                Epoch    2 Train error : 0.00190743 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61.9%  lr: 0.013944  loss: 0.001528  eta: 0h3m  tot: 0h10m45s  (72.4%)%  lr: 0.019890  loss: 0.001467  eta: 0h5m  tot: 0h9m3s  (60.3%)1.3%  lr: 0.019880  loss: 0.001457  eta: 0h6m  tot: 0h9m4s  (60.3%)1.7%  lr: 0.019840  loss: 0.001348  eta: 0h5m  tot: 0h9m4s  (60.3%)3.0%  lr: 0.019760  loss: 0.001325  eta: 0h5m  tot: 0h9m6s  (60.6%)3.1%  lr: 0.019760  loss: 0.001363  eta: 0h5m  tot: 0h9m6s  (60.6%)4.5%  lr: 0.019620  loss: 0.001431  eta: 0h5m  tot: 0h9m8s  (60.9%)5.0%  lr: 0.019570  loss: 0.001444  eta: 0h5m  tot: 0h9m9s  (61.0%)%  lr: 0.019510  loss: 0.001415  eta: 0h5m  tot: 0h9m10s  (61.1%)5.7%  lr: 0.019479  loss: 0.001420  eta: 0h5m  tot: 0h9m11s  (61.1%)6.0%  lr: 0.019439  loss: 0.001395  eta: 0h5m  tot: 0h9m11s  (61.2%)8.0%  lr: 0.019229  loss: 0.001442  eta: 0h5m  tot: 0h9m15s  (61.6%)8.9%  lr: 0.019119  loss: 0.001428  eta: 0h5m  tot: 0h9m16s  (61.8%)9.3%  lr: 0.019079  loss: 0.001414  eta: 0h5m  tot: 0h9m17s  (61.9%)9.5%  lr: 0.019069  loss: 0.001414  eta: 0h5m  tot: 0h9m17s  (61.9%)10.0%  lr: 0.019029  loss: 0.001428  eta: 0h5m  tot: 0h9m18s  (62.0%)10.5%  lr: 0.018989  loss: 0.001436  eta: 0h5m  tot: 0h9m19s  (62.1%)10.7%  lr: 0.018969  loss: 0.001443  eta: 0h5m  tot: 0h9m19s  (62.1%)11.1%  lr: 0.018949  loss: 0.001451  eta: 0h5m  tot: 0h9m20s  (62.2%)11.5%  lr: 0.018939  loss: 0.001462  eta: 0h5m  tot: 0h9m21s  (62.3%)12.0%  lr: 0.018899  loss: 0.001470  eta: 0h5m  tot: 0h9m21s  (62.4%)12.1%  lr: 0.018899  loss: 0.001464  eta: 0h5m  tot: 0h9m21s  (62.4%)12.2%  lr: 0.018879  loss: 0.001460  eta: 0h5m  tot: 0h9m22s  (62.4%)12.7%  lr: 0.018819  loss: 0.001458  eta: 0h5m  tot: 0h9m22s  (62.5%)12.9%  lr: 0.018789  loss: 0.001462  eta: 0h5m  tot: 0h9m23s  (62.6%)13.0%  lr: 0.018789  loss: 0.001467  eta: 0h5m  tot: 0h9m23s  (62.6%)13.1%  lr: 0.018779  loss: 0.001474  eta: 0h5m  tot: 0h9m23s  (62.6%)13.1%  lr: 0.018759  loss: 0.001473  eta: 0h5m  tot: 0h9m23s  (62.6%)13.2%  lr: 0.018749  loss: 0.001475  eta: 0h5m  tot: 0h9m23s  (62.6%)13.5%  lr: 0.018739  loss: 0.001478  eta: 0h5m  tot: 0h9m24s  (62.7%)14.1%  lr: 0.018619  loss: 0.001504  eta: 0h5m  tot: 0h9m25s  (62.8%)15.0%  lr: 0.018499  loss: 0.001512  eta: 0h5m  tot: 0h9m27s  (63.0%)15.2%  lr: 0.018468  loss: 0.001516  eta: 0h5m  tot: 0h9m27s  (63.0%)15.7%  lr: 0.018378  loss: 0.001533  eta: 0h5m  tot: 0h9m28s  (63.1%)16.4%  lr: 0.018358  loss: 0.001536  eta: 0h5m  tot: 0h9m29s  (63.3%)16.5%  lr: 0.018358  loss: 0.001531  eta: 0h5m  tot: 0h9m30s  (63.3%)16.7%  lr: 0.018318  loss: 0.001539  eta: 0h5m  tot: 0h9m30s  (63.3%)17.8%  lr: 0.018198  loss: 0.001547  eta: 0h5m  tot: 0h9m32s  (63.6%)17.9%  lr: 0.018188  loss: 0.001543  eta: 0h5m  tot: 0h9m32s  (63.6%)18.3%  lr: 0.018148  loss: 0.001529  eta: 0h5m  tot: 0h9m32s  (63.7%)18.8%  lr: 0.018108  loss: 0.001515  eta: 0h5m  tot: 0h9m33s  (63.8%)19.6%  lr: 0.018018  loss: 0.001535  eta: 0h5m  tot: 0h9m35s  (63.9%)20.6%  lr: 0.017908  loss: 0.001533  eta: 0h5m  tot: 0h9m36s  (64.1%)20.8%  lr: 0.017898  loss: 0.001530  eta: 0h5m  tot: 0h9m37s  (64.2%)21.2%  lr: 0.017858  loss: 0.001532  eta: 0h5m  tot: 0h9m37s  (64.2%)21.7%  lr: 0.017768  loss: 0.001530  eta: 0h5m  tot: 0h9m38s  (64.3%)22.1%  lr: 0.017728  loss: 0.001532  eta: 0h5m  tot: 0h9m39s  (64.4%)22.5%  lr: 0.017678  loss: 0.001531  eta: 0h5m  tot: 0h9m40s  (64.5%)23.1%  lr: 0.017608  loss: 0.001533  eta: 0h5m  tot: 0h9m41s  (64.6%)23.5%  lr: 0.017598  loss: 0.001527  eta: 0h5m  tot: 0h9m41s  (64.7%)24.6%  lr: 0.017508  loss: 0.001517  eta: 0h4m  tot: 0h9m43s  (64.9%)24.7%  lr: 0.017498  loss: 0.001516  eta: 0h4m  tot: 0h9m43s  (64.9%)26.5%  lr: 0.017267  loss: 0.001538  eta: 0h4m  tot: 0h9m46s  (65.3%)27.5%  lr: 0.017207  loss: 0.001535  eta: 0h4m  tot: 0h9m48s  (65.5%)27.6%  lr: 0.017197  loss: 0.001531  eta: 0h4m  tot: 0h9m48s  (65.5%)28.1%  lr: 0.017147  loss: 0.001528  eta: 0h4m  tot: 0h9m49s  (65.6%)29.2%  lr: 0.017047  loss: 0.001535  eta: 0h4m  tot: 0h9m51s  (65.8%)30.8%  lr: 0.016877  loss: 0.001539  eta: 0h4m  tot: 0h9m53s  (66.2%)%  lr: 0.016877  loss: 0.001541  eta: 0h4m  tot: 0h9m53s  (66.2%)31.1%  lr: 0.016857  loss: 0.001541  eta: 0h4m  tot: 0h9m54s  (66.2%)31.4%  lr: 0.016827  loss: 0.001542  eta: 0h4m  tot: 0h9m54s  (66.3%)%  lr: 0.016797  loss: 0.001540  eta: 0h4m  tot: 0h9m54s  (66.3%)32.6%  lr: 0.016707  loss: 0.001527  eta: 0h4m  tot: 0h9m56s  (66.5%)32.9%  lr: 0.016687  loss: 0.001534  eta: 0h4m  tot: 0h9m56s  (66.6%)33.0%  lr: 0.016667  loss: 0.001533  eta: 0h4m  tot: 0h9m57s  (66.6%)33.8%  lr: 0.016627  loss: 0.001538  eta: 0h4m  tot: 0h9m58s  (66.8%)34.1%  lr: 0.016607  loss: 0.001541  eta: 0h4m  tot: 0h9m58s  (66.8%)34.6%  lr: 0.016557  loss: 0.001542  eta: 0h4m  tot: 0h9m59s  (66.9%)34.7%  lr: 0.016527  loss: 0.001537  eta: 0h4m  tot: 0h9m59s  (66.9%)35.0%  lr: 0.016447  loss: 0.001538  eta: 0h4m  tot: 0h10m0s  (67.0%)35.8%  lr: 0.016336  loss: 0.001548  eta: 0h4m  tot: 0h10m1s  (67.2%)36.3%  lr: 0.016296  loss: 0.001541  eta: 0h4m  tot: 0h10m2s  (67.3%)36.7%  lr: 0.016266  loss: 0.001535  eta: 0h4m  tot: 0h10m3s  (67.3%)37.6%  lr: 0.016146  loss: 0.001538  eta: 0h4m  tot: 0h10m5s  (67.5%)38.3%  lr: 0.016056  loss: 0.001542  eta: 0h4m  tot: 0h10m6s  (67.7%)39.8%  lr: 0.015906  loss: 0.001547  eta: 0h4m  tot: 0h10m8s  (68.0%)40.3%  lr: 0.015866  loss: 0.001547  eta: 0h4m  tot: 0h10m9s  (68.1%)40.8%  lr: 0.015826  loss: 0.001538  eta: 0h4m  tot: 0h10m10s  (68.2%)41.3%  lr: 0.015796  loss: 0.001533  eta: 0h4m  tot: 0h10m11s  (68.3%)41.5%  lr: 0.015776  loss: 0.001533  eta: 0h4m  tot: 0h10m11s  (68.3%) (68.4%)42.3%  lr: 0.015656  loss: 0.001535  eta: 0h4m  tot: 0h10m13s  (68.5%)42.4%  lr: 0.015646  loss: 0.001535  eta: 0h4m  tot: 0h10m13s  (68.5%)43.1%  lr: 0.015526  loss: 0.001524  eta: 0h4m  tot: 0h10m14s  (68.6%)43.6%  lr: 0.015516  loss: 0.001521  eta: 0h4m  tot: 0h10m15s  (68.7%)43.8%  lr: 0.015516  loss: 0.001524  eta: 0h4m  tot: 0h10m15s  (68.8%)44.6%  lr: 0.015476  loss: 0.001525  eta: 0h4m  tot: 0h10m16s  (68.9%)45.1%  lr: 0.015425  loss: 0.001527  eta: 0h4m  tot: 0h10m17s  (69.0%)45.6%  lr: 0.015375  loss: 0.001522  eta: 0h4m  tot: 0h10m18s  (69.1%)45.9%  lr: 0.015355  loss: 0.001523  eta: 0h4m  tot: 0h10m18s  (69.2%)%  lr: 0.015345  loss: 0.001521  eta: 0h4m  tot: 0h10m19s  (69.2%)46.8%  lr: 0.015305  loss: 0.001517  eta: 0h4m  tot: 0h10m20s  (69.4%)47.1%  lr: 0.015295  loss: 0.001517  eta: 0h4m  tot: 0h10m20s  (69.4%)47.6%  lr: 0.015265  loss: 0.001525  eta: 0h4m  tot: 0h10m21s  (69.5%)69.6%)48.5%  lr: 0.015185  loss: 0.001528  eta: 0h4m  tot: 0h10m22s  (69.7%)48.7%  lr: 0.015175  loss: 0.001533  eta: 0h4m  tot: 0h10m23s  (69.7%)48.8%  lr: 0.015165  loss: 0.001531  eta: 0h4m  tot: 0h10m23s  (69.8%)49.2%  lr: 0.015135  loss: 0.001530  eta: 0h4m  tot: 0h10m24s  (69.8%)49.2%  lr: 0.015125  loss: 0.001528  eta: 0h4m  tot: 0h10m24s  (69.8%)49.3%  lr: 0.015105  loss: 0.001526  eta: 0h4m  tot: 0h10m24s  (69.9%)49.5%  lr: 0.015075  loss: 0.001526  eta: 0h4m  tot: 0h10m24s  (69.9%)49.7%  lr: 0.015035  loss: 0.001524  eta: 0h4m  tot: 0h10m25s  (69.9%)50.1%  lr: 0.014975  loss: 0.001530  eta: 0h4m  tot: 0h10m25s  (70.0%)50.4%  lr: 0.014955  loss: 0.001526  eta: 0h4m  tot: 0h10m26s  (70.1%)50.8%  lr: 0.014935  loss: 0.001527  eta: 0h4m  tot: 0h10m26s  (70.2%)51.0%  lr: 0.014905  loss: 0.001525  eta: 0h4m  tot: 0h10m27s  (70.2%)51.8%  lr: 0.014865  loss: 0.001529  eta: 0h4m  tot: 0h10m28s  (70.4%)%  lr: 0.014855  loss: 0.001528  eta: 0h4m  tot: 0h10m28s  (70.4%)53.3%  lr: 0.014755  loss: 0.001526  eta: 0h4m  tot: 0h10m31s  (70.7%)53.6%  lr: 0.014745  loss: 0.001524  eta: 0h4m  tot: 0h10m31s  (70.7%)55.8%  lr: 0.014605  loss: 0.001526  eta: 0h4m  tot: 0h10m35s  (71.2%)56.5%  lr: 0.014545  loss: 0.001523  eta: 0h4m  tot: 0h10m36s  (71.3%)57.8%  lr: 0.014445  loss: 0.001520  eta: 0h3m  tot: 0h10m38s  (71.6%)58.4%  lr: 0.014334  loss: 0.001519  eta: 0h3m  tot: 0h10m39s  (71.7%)58.7%  lr: 0.014304  loss: 0.001517  eta: 0h3m  tot: 0h10m40s  (71.7%)59.1%  lr: 0.014264  loss: 0.001523  eta: 0h3m  tot: 0h10m41s  (71.8%)59.4%  lr: 0.014244  loss: 0.001527  eta: 0h3m  tot: 0h10m41s  (71.9%)59.8%  lr: 0.014204  loss: 0.001529  eta: 0h3m  tot: 0h10m41s  (72.0%)60.0%  lr: 0.014174  loss: 0.001530  eta: 0h3m  tot: 0h10m42s  (72.0%)62.0%  lr: 0.013944  loss: 0.001528  eta: 0h3m  tot: 0h10m45s  (72.4%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010330  loss: 0.001544  eta: 0h2m  tot: 0h11m47  (80.0%)62.5%  lr: 0.013884  loss: 0.001527  eta: 0h3m  tot: 0h10m46s  (72.5%)%  lr: 0.013854  loss: 0.001527  eta: 0h3m  tot: 0h10m47s  (72.6%)63.2%  lr: 0.013854  loss: 0.001528  eta: 0h3m  tot: 0h10m47s  (72.6%)63.3%  lr: 0.013854  loss: 0.001529  eta: 0h3m  tot: 0h10m47s  (72.7%)63.9%  lr: 0.013804  loss: 0.001537  eta: 0h3m  tot: 0h10m48s  (72.8%)64.0%  lr: 0.013804  loss: 0.001539  eta: 0h3m  tot: 0h10m48s  (72.8%)64.5%  lr: 0.013774  loss: 0.001541  eta: 0h3m  tot: 0h10m49s  (72.9%)64.9%  lr: 0.013744  loss: 0.001541  eta: 0h3m  tot: 0h10m50s  (73.0%)65.0%  lr: 0.013744  loss: 0.001542  eta: 0h3m  tot: 0h10m50s  (73.0%)65.4%  lr: 0.013664  loss: 0.001537  eta: 0h3m  tot: 0h10m51s  (73.1%)65.5%  lr: 0.013654  loss: 0.001536  eta: 0h3m  tot: 0h10m51s  (73.1%)65.7%  lr: 0.013644  loss: 0.001541  eta: 0h3m  tot: 0h10m52s  (73.1%)66.9%  lr: 0.013534  loss: 0.001538  eta: 0h3m  tot: 0h10m54s  (73.4%)  lr: 0.013474  loss: 0.001536  eta: 0h3m  tot: 0h10m54s  (73.4%)10m54s  (73.4%)67.9%  lr: 0.013393  loss: 0.001537  eta: 0h3m  tot: 0h10m55s  (73.6%)68.0%  lr: 0.013383  loss: 0.001536  eta: 0h3m  tot: 0h10m55s  (73.6%)%  lr: 0.013213  loss: 0.001536  eta: 0h3m  tot: 0h10m57s  (73.8%)70.2%  lr: 0.013123  loss: 0.001536  eta: 0h3m  tot: 0h10m59s  (74.0%)70.8%  lr: 0.013093  loss: 0.001533  eta: 0h3m  tot: 0h11m0s  (74.2%)71.7%  lr: 0.013003  loss: 0.001535  eta: 0h3m  tot: 0h11m1s  (74.3%)71.9%  lr: 0.012983  loss: 0.001535  eta: 0h3m  tot: 0h11m2s  (74.4%)72.2%  lr: 0.012953  loss: 0.001538  eta: 0h3m  tot: 0h11m2s  (74.4%)73.8%  lr: 0.012883  loss: 0.001533  eta: 0h3m  tot: 0h11m4s  (74.8%)74.2%  lr: 0.012853  loss: 0.001533  eta: 0h3m  tot: 0h11m5s  (74.8%)75.3%  lr: 0.012793  loss: 0.001536  eta: 0h3m  tot: 0h11m7s  (75.1%)76.0%  lr: 0.012743  loss: 0.001541  eta: 0h3m  tot: 0h11m8s  (75.2%)76.7%  lr: 0.012683  loss: 0.001539  eta: 0h3m  tot: 0h11m9s  (75.3%)76.9%  lr: 0.012663  loss: 0.001542  eta: 0h3m  tot: 0h11m10s  (75.4%)77.1%  lr: 0.012643  loss: 0.001540  eta: 0h3m  tot: 0h11m10s  (75.4%)77.4%  lr: 0.012603  loss: 0.001541  eta: 0h3m  tot: 0h11m11s  (75.5%)77.6%  lr: 0.012573  loss: 0.001540  eta: 0h3m  tot: 0h11m11s  (75.5%)78.0%  lr: 0.012553  loss: 0.001540  eta: 0h3m  tot: 0h11m12s  (75.6%)78.1%  lr: 0.012523  loss: 0.001538  eta: 0h3m  tot: 0h11m12s  (75.6%)78.5%  lr: 0.012503  loss: 0.001535  eta: 0h3m  tot: 0h11m12s  (75.7%)78.6%  lr: 0.012503  loss: 0.001535  eta: 0h3m  tot: 0h11m12s  (75.7%)13s  (75.8%)79.2%  lr: 0.012413  loss: 0.001534  eta: 0h3m  tot: 0h11m14s  (75.8%)s  (76.2%)82.3%  lr: 0.012102  loss: 0.001535  eta: 0h3m  tot: 0h11m19s  (76.5%)82.4%  lr: 0.012102  loss: 0.001535  eta: 0h3m  tot: 0h11m19s  (76.5%)82.5%  lr: 0.012102  loss: 0.001536  eta: 0h3m  tot: 0h11m19s  (76.5%)82.9%  lr: 0.012032  loss: 0.001538  eta: 0h3m  tot: 0h11m20s  (76.6%)84.2%  lr: 0.011882  loss: 0.001537  eta: 0h3m  tot: 0h11m22s  (76.8%)85.1%  lr: 0.011832  loss: 0.001538  eta: 0h3m  tot: 0h11m23s  (77.0%)85.3%  lr: 0.011822  loss: 0.001538  eta: 0h3m  tot: 0h11m24s  (77.1%)85.8%  lr: 0.011782  loss: 0.001537  eta: 0h3m  tot: 0h11m24s  (77.2%)86.7%  lr: 0.011642  loss: 0.001538  eta: 0h3m  tot: 0h11m26s  (77.3%)87.0%  lr: 0.011632  loss: 0.001537  eta: 0h3m  tot: 0h11m26s  (77.4%)87.5%  lr: 0.011602  loss: 0.001535  eta: 0h3m  tot: 0h11m27s  (77.5%)88.7%  lr: 0.011462  loss: 0.001536  eta: 0h3m  tot: 0h11m29s  (77.7%)88.9%  lr: 0.011462  loss: 0.001536  eta: 0h3m  tot: 0h11m30s  (77.8%)89.6%  lr: 0.011432  loss: 0.001539  eta: 0h3m  tot: 0h11m30s  (77.9%)90.9%  lr: 0.011281  loss: 0.001537  eta: 0h3m  tot: 0h11m33s  (78.2%)91.1%  lr: 0.011261  loss: 0.001536  eta: 0h3m  tot: 0h11m33s  (78.2%)91.7%  lr: 0.011221  loss: 0.001535  eta: 0h3m  tot: 0h11m34s  (78.3%)  loss: 0.001535  eta: 0h3m  tot: 0h11m34s  (78.4%)92.1%  lr: 0.011171  loss: 0.001534  eta: 0h3m  tot: 0h11m35s  (78.4%)93.3%  lr: 0.011071  loss: 0.001530  eta: 0h2m  tot: 0h11m37s  (78.7%)93.5%  lr: 0.011061  loss: 0.001531  eta: 0h2m  tot: 0h11m37s  (78.7%)93.8%  lr: 0.011011  loss: 0.001531  eta: 0h2m  tot: 0h11m37s  (78.8%)94.1%  lr: 0.010991  loss: 0.001534  eta: 0h2m  tot: 0h11m38s  (78.8%)94.6%  lr: 0.010921  loss: 0.001533  eta: 0h2m  tot: 0h11m39s  (78.9%)94.7%  lr: 0.010921  loss: 0.001533  eta: 0h2m  tot: 0h11m39s  (78.9%)11m41s  (79.2%)96.3%  lr: 0.010751  loss: 0.001533  eta: 0h2m  tot: 0h11m42s  (79.3%)96.5%  lr: 0.010721  loss: 0.001533  eta: 0h2m  tot: 0h11m42s  (79.3%)97.0%  lr: 0.010661  loss: 0.001540  eta: 0h2m  tot: 0h11m43s  (79.4%)97.3%  lr: 0.010611  loss: 0.001539  eta: 0h2m  tot: 0h11m43s  (79.5%)98.8%  lr: 0.010481  loss: 0.001538  eta: 0h2m  tot: 0h11m46s  (79.8%)98.8%  lr: 0.010461  loss: 0.001537  eta: 0h2m  tot: 0h11m46s  (79.8%)99.6%  lr: 0.010381  loss: 0.001540  eta: 0h2m  tot: 0h11m47s  (79.9%)99.9%  lr: 0.010330  loss: 0.001545  eta: 0h2m  tot: 0h11m47s  (80.0%)s  (80.0%)\n",
      " ---+++                Epoch    3 Train error : 0.00155826 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49.9%  lr: 0.004995  loss: 0.001342  eta: 0h1m  tot: 0h13m22s  (90.0%).2%  lr: 0.009990  loss: 0.001668  eta: 0h4m  tot: 0h11m53s  (80.0%)1.4%  lr: 0.009890  loss: 0.001186  eta: 0h2m  tot: 0h11m55s  (80.3%)1.5%  lr: 0.009880  loss: 0.001169  eta: 0h2m  tot: 0h11m55s  (80.3%)1.6%  lr: 0.009880  loss: 0.001175  eta: 0h2m  tot: 0h11m56s  (80.3%)2.0%  lr: 0.009840  loss: 0.001261  eta: 0h2m  tot: 0h11m56s  (80.4%)3.0%  lr: 0.009690  loss: 0.001307  eta: 0h2m  tot: 0h11m58s  (80.6%)0.001467  eta: 0h2m  tot: 0h11m59s  (80.7%)4.2%  lr: 0.009550  loss: 0.001450  eta: 0h2m  tot: 0h12m0s  (80.8%)4.3%  lr: 0.009550  loss: 0.001429  eta: 0h2m  tot: 0h12m0s  (80.9%)4.5%  lr: 0.009550  loss: 0.001402  eta: 0h2m  tot: 0h12m1s  (80.9%)5.0%  lr: 0.009510  loss: 0.001428  eta: 0h2m  tot: 0h12m1s  (81.0%)5.4%  lr: 0.009469  loss: 0.001386  eta: 0h2m  tot: 0h12m2s  (81.1%)6.4%  lr: 0.009399  loss: 0.001422  eta: 0h2m  tot: 0h12m4s  (81.3%)6.5%  lr: 0.009389  loss: 0.001440  eta: 0h2m  tot: 0h12m4s  (81.3%)6.7%  lr: 0.009379  loss: 0.001446  eta: 0h2m  tot: 0h12m4s  (81.3%)6.8%  lr: 0.009369  loss: 0.001443  eta: 0h2m  tot: 0h12m5s  (81.4%)6.9%  lr: 0.009369  loss: 0.001431  eta: 0h2m  tot: 0h12m5s  (81.4%)%  lr: 0.009349  loss: 0.001414  eta: 0h2m  tot: 0h12m5s  (81.4%)7.4%  lr: 0.009349  loss: 0.001397  eta: 0h2m  tot: 0h12m5s  (81.5%)8.1%  lr: 0.009249  loss: 0.001359  eta: 0h2m  tot: 0h12m7s  (81.6%)8.4%  lr: 0.009209  loss: 0.001346  eta: 0h2m  tot: 0h12m7s  (81.7%)8.5%  lr: 0.009189  loss: 0.001336  eta: 0h2m  tot: 0h12m7s  (81.7%)8.6%  lr: 0.009179  loss: 0.001334  eta: 0h2m  tot: 0h12m8s  (81.7%)8.7%  lr: 0.009169  loss: 0.001349  eta: 0h2m  tot: 0h12m8s  (81.7%)8.8%  lr: 0.009159  loss: 0.001344  eta: 0h2m  tot: 0h12m8s  (81.8%)h2m  tot: 0h12m8s  (81.8%)9.5%  lr: 0.009109  loss: 0.001335  eta: 0h2m  tot: 0h12m9s  (81.9%)9.8%  lr: 0.009099  loss: 0.001348  eta: 0h2m  tot: 0h12m10s  (82.0%)10.2%  lr: 0.009079  loss: 0.001349  eta: 0h2m  tot: 0h12m11s  (82.0%)10.5%  lr: 0.009079  loss: 0.001348  eta: 0h2m  tot: 0h12m11s  (82.1%)13.0%  lr: 0.008839  loss: 0.001368  eta: 0h2m  tot: 0h12m15s  (82.6%)13.2%  lr: 0.008809  loss: 0.001378  eta: 0h2m  tot: 0h12m16s  (82.6%)13.5%  lr: 0.008799  loss: 0.001375  eta: 0h2m  tot: 0h12m16s  (82.7%)  lr: 0.008669  loss: 0.001385  eta: 0h2m  tot: 0h12m17s  (82.8%)14.3%  lr: 0.008639  loss: 0.001379  eta: 0h2m  tot: 0h12m18s  (82.9%)14.5%  lr: 0.008589  loss: 0.001383  eta: 0h2m  tot: 0h12m18s  (82.9%)14.8%  lr: 0.008539  loss: 0.001380  eta: 0h2m  tot: 0h12m19s  (83.0%)14.9%  lr: 0.008529  loss: 0.001375  eta: 0h2m  tot: 0h12m19s  (83.0%)14.9%  lr: 0.008519  loss: 0.001374  eta: 0h2m  tot: 0h12m19s  (83.0%)15.2%  lr: 0.008509  loss: 0.001380  eta: 0h2m  tot: 0h12m19s  (83.0%)15.8%  lr: 0.008468  loss: 0.001371  eta: 0h2m  tot: 0h12m20s  (83.2%)16.2%  lr: 0.008428  loss: 0.001368  eta: 0h2m  tot: 0h12m21s  (83.2%)16.7%  lr: 0.008398  loss: 0.001368  eta: 0h2m  tot: 0h12m22s  (83.3%)16.9%  lr: 0.008368  loss: 0.001357  eta: 0h2m  tot: 0h12m22s  (83.4%)17.3%  lr: 0.008348  loss: 0.001359  eta: 0h2m  tot: 0h12m23s  (83.5%)17.7%  lr: 0.008288  loss: 0.001347  eta: 0h2m  tot: 0h12m23s  (83.5%)17.9%  lr: 0.008258  loss: 0.001342  eta: 0h2m  tot: 0h12m24s  (83.6%)18.0%  lr: 0.008258  loss: 0.001342  eta: 0h2m  tot: 0h12m24s  (83.6%)18.1%  lr: 0.008238  loss: 0.001340  eta: 0h2m  tot: 0h12m24s  (83.6%)18.3%  lr: 0.008208  loss: 0.001337  eta: 0h2m  tot: 0h12m24s  (83.7%)18.4%  lr: 0.008178  loss: 0.001331  eta: 0h2m  tot: 0h12m24s  (83.7%)18.9%  lr: 0.008088  loss: 0.001328  eta: 0h2m  tot: 0h12m25s  (83.8%)19.3%  lr: 0.008018  loss: 0.001330  eta: 0h2m  tot: 0h12m26s  (83.9%)20.3%  lr: 0.007928  loss: 0.001331  eta: 0h2m  tot: 0h12m27s  (84.1%)20.3%  lr: 0.007908  loss: 0.001333  eta: 0h2m  tot: 0h12m28s  (84.1%)21.3%  lr: 0.007768  loss: 0.001348  eta: 0h2m  tot: 0h12m29s  (84.3%)21.9%  lr: 0.007738  loss: 0.001343  eta: 0h2m  tot: 0h12m30s  (84.4%)22.1%  lr: 0.007718  loss: 0.001337  eta: 0h2m  tot: 0h12m31s  (84.4%)22.9%  lr: 0.007678  loss: 0.001337  eta: 0h2m  tot: 0h12m32s  (84.6%)23.0%  lr: 0.007668  loss: 0.001334  eta: 0h2m  tot: 0h12m32s  (84.6%)23.0%  lr: 0.007668  loss: 0.001331  eta: 0h2m  tot: 0h12m32s  (84.6%)23.5%  lr: 0.007608  loss: 0.001330  eta: 0h2m  tot: 0h12m33s  (84.7%)25.8%  lr: 0.007377  loss: 0.001325  eta: 0h2m  tot: 0h12m36s  (85.2%)38s  (85.4%)27.8%  lr: 0.007237  loss: 0.001323  eta: 0h2m  tot: 0h12m40s  (85.6%)28.1%  lr: 0.007237  loss: 0.001325  eta: 0h2m  tot: 0h12m40s  (85.6%)28.2%  lr: 0.007237  loss: 0.001324  eta: 0h2m  tot: 0h12m41s  (85.6%)28.4%  lr: 0.007217  loss: 0.001327  eta: 0h2m  tot: 0h12m41s  (85.7%)28.5%  lr: 0.007197  loss: 0.001325  eta: 0h2m  tot: 0h12m41s  (85.7%)28.6%  lr: 0.007187  loss: 0.001324  eta: 0h2m  tot: 0h12m41s  (85.7%)29.0%  lr: 0.007107  loss: 0.001325  eta: 0h2m  tot: 0h12m42s  (85.8%)29.4%  lr: 0.007077  loss: 0.001326  eta: 0h1m  tot: 0h12m43s  (85.9%)30.0%  lr: 0.007017  loss: 0.001334  eta: 0h1m  tot: 0h12m44s  (86.0%)30.5%  lr: 0.006977  loss: 0.001332  eta: 0h1m  tot: 0h12m44s  (86.1%)30.8%  lr: 0.006947  loss: 0.001341  eta: 0h1m  tot: 0h12m45s  (86.2%)31.0%  lr: 0.006907  loss: 0.001345  eta: 0h1m  tot: 0h12m45s  (86.2%)31.1%  lr: 0.006907  loss: 0.001345  eta: 0h1m  tot: 0h12m46s  (86.2%)32.2%  lr: 0.006727  loss: 0.001339  eta: 0h1m  tot: 0h12m48s  (86.4%)0.006717  loss: 0.001338  eta: 0h1m  tot: 0h12m49s  (86.5%)32.8%  lr: 0.006707  loss: 0.001342  eta: 0h1m  tot: 0h12m49s  (86.6%)32.9%  lr: 0.006687  loss: 0.001342  eta: 0h1m  tot: 0h12m49s  (86.6%)33.4%  lr: 0.006667  loss: 0.001338  eta: 0h1m  tot: 0h12m50s  (86.7%)34.1%  lr: 0.006547  loss: 0.001333  eta: 0h1m  tot: 0h12m51s  (86.8%)35.5%  lr: 0.006477  loss: 0.001325  eta: 0h1m  tot: 0h12m55s  (87.1%)35.7%  lr: 0.006426  loss: 0.001326  eta: 0h1m  tot: 0h12m56s  (87.1%)%  lr: 0.006416  loss: 0.001327  eta: 0h1m  tot: 0h12m56s  (87.2%)36.5%  lr: 0.006326  loss: 0.001328  eta: 0h1m  tot: 0h12m57s  (87.3%)37.5%  lr: 0.006246  loss: 0.001331  eta: 0h1m  tot: 0h13m0s  (87.5%)37.7%  lr: 0.006246  loss: 0.001328  eta: 0h1m  tot: 0h13m1s  (87.5%)%  lr: 0.006236  loss: 0.001334  eta: 0h1m  tot: 0h13m1s  (87.6%)38.3%  lr: 0.006206  loss: 0.001332  eta: 0h1m  tot: 0h13m2s  (87.7%)38.5%  lr: 0.006186  loss: 0.001337  eta: 0h1m  tot: 0h13m2s  (87.7%)%  lr: 0.006186  loss: 0.001340  eta: 0h1m  tot: 0h13m2s  (87.7%)38.8%  lr: 0.006166  loss: 0.001338  eta: 0h1m  tot: 0h13m3s  (87.8%)39.0%  lr: 0.006156  loss: 0.001338  eta: 0h1m  tot: 0h13m3s  (87.8%)39.2%  lr: 0.006146  loss: 0.001337  eta: 0h1m  tot: 0h13m3s  (87.8%)39.4%  lr: 0.006136  loss: 0.001334  eta: 0h1m  tot: 0h13m4s  (87.9%)39.8%  lr: 0.006086  loss: 0.001334  eta: 0h1m  tot: 0h13m4s  (88.0%)39.9%  lr: 0.006076  loss: 0.001332  eta: 0h1m  tot: 0h13m4s  (88.0%)40.7%  lr: 0.005986  loss: 0.001330  eta: 0h1m  tot: 0h13m6s  (88.1%)41.0%  lr: 0.005946  loss: 0.001325  eta: 0h1m  tot: 0h13m6s  (88.2%)41.3%  lr: 0.005906  loss: 0.001324  eta: 0h1m  tot: 0h13m7s  (88.3%)41.6%  lr: 0.005886  loss: 0.001327  eta: 0h1m  tot: 0h13m7s  (88.3%)41.7%  lr: 0.005886  loss: 0.001326  eta: 0h1m  tot: 0h13m8s  (88.3%)41.9%  lr: 0.005886  loss: 0.001325  eta: 0h1m  tot: 0h13m8s  (88.4%)42.0%  lr: 0.005856  loss: 0.001324  eta: 0h1m  tot: 0h13m8s  (88.4%)42.3%  lr: 0.005846  loss: 0.001327  eta: 0h1m  tot: 0h13m9s  (88.5%)42.9%  lr: 0.005816  loss: 0.001329  eta: 0h1m  tot: 0h13m10s  (88.6%)43.0%  lr: 0.005806  loss: 0.001330  eta: 0h1m  tot: 0h13m10s  (88.6%)43.4%  lr: 0.005766  loss: 0.001331  eta: 0h1m  tot: 0h13m10s  (88.7%)43.8%  lr: 0.005676  loss: 0.001344  eta: 0h1m  tot: 0h13m11s  (88.8%)44.0%  lr: 0.005626  loss: 0.001345  eta: 0h1m  tot: 0h13m12s  (88.8%)45.5%  lr: 0.005476  loss: 0.001338  eta: 0h1m  tot: 0h13m14s  (89.1%)45.7%  lr: 0.005425  loss: 0.001337  eta: 0h1m  tot: 0h13m15s  (89.1%)46.1%  lr: 0.005375  loss: 0.001339  eta: 0h1m  tot: 0h13m15s  (89.2%)47.4%  lr: 0.005225  loss: 0.001338  eta: 0h1m  tot: 0h13m17s  (89.5%)48.1%  lr: 0.005165  loss: 0.001342  eta: 0h1m  tot: 0h13m19s  (89.6%)%  lr: 0.005125  loss: 0.001338  eta: 0h1m  tot: 0h13m19s  (89.7%)49.6%  lr: 0.004995  loss: 0.001340  eta: 0h1m  tot: 0h13m21s  (89.9%)50.0%  lr: 0.004985  loss: 0.001344  eta: 0h1m  tot: 0h13m22s  (90.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000260  loss: 0.001361  eta: <1min   tot: 0h14m45s  (100.0%)%  lr: 0.004855  loss: 0.001348  eta: 0h1m  tot: 0h13m23s  (90.2%)51.9%  lr: 0.004745  loss: 0.001347  eta: 0h1m  tot: 0h13m25s  (90.4%)53.2%  lr: 0.004635  loss: 0.001350  eta: 0h1m  tot: 0h13m27s  (90.6%)53.6%  lr: 0.004605  loss: 0.001352  eta: 0h1m  tot: 0h13m28s  (90.7%)53.7%  lr: 0.004585  loss: 0.001352  eta: 0h1m  tot: 0h13m28s  (90.7%)54.4%  lr: 0.004515  loss: 0.001353  eta: 0h1m  tot: 0h13m29s  (90.9%)m  tot: 0h13m30s  (90.9%)54.9%  lr: 0.004485  loss: 0.001351  eta: 0h1m  tot: 0h13m30s  (91.0%)%  lr: 0.004425  loss: 0.001348  eta: 0h1m  tot: 0h13m31s  (91.1%)56.4%  lr: 0.004324  loss: 0.001346  eta: 0h1m  tot: 0h13m34s  (91.3%)%  lr: 0.004304  loss: 0.001344  eta: 0h1m  tot: 0h13m34s  (91.3%)56.9%  lr: 0.004284  loss: 0.001343  eta: 0h1m  tot: 0h13m35s  (91.4%)58.3%  lr: 0.004144  loss: 0.001341  eta: 0h1m  tot: 0h13m37s  (91.7%)59.1%  lr: 0.004084  loss: 0.001346  eta: 0h1m  tot: 0h13m39s  (91.8%)59.7%  lr: 0.004034  loss: 0.001346  eta: 0h1m  tot: 0h13m40s  (91.9%)60.2%  lr: 0.003964  loss: 0.001350  eta: 0h1m  tot: 0h13m41s  (92.0%)61.1%  lr: 0.003914  loss: 0.001352  eta: 0h1m  tot: 0h13m42s  (92.2%)61.8%  lr: 0.003854  loss: 0.001357  eta: 0h1m  tot: 0h13m43s  (92.4%)61.8%  lr: 0.003844  loss: 0.001356  eta: 0h1m  tot: 0h13m44s  (92.4%)62.6%  lr: 0.003774  loss: 0.001356  eta: 0h1m  tot: 0h13m45s  (92.5%)62.8%  lr: 0.003774  loss: 0.001356  eta: 0h1m  tot: 0h13m45s  (92.6%)%  lr: 0.003694  loss: 0.001352  eta: 0h1m  tot: 0h13m46s  (92.7%)64.3%  lr: 0.003634  loss: 0.001354  eta: 0h1m  tot: 0h13m47s  (92.9%)64.5%  lr: 0.003614  loss: 0.001352  eta: 0h1m  tot: 0h13m48s  (92.9%)64.8%  lr: 0.003594  loss: 0.001354  eta: 0h1m  tot: 0h13m48s  (93.0%)65.1%  lr: 0.003564  loss: 0.001352  eta: 0h1m  tot: 0h13m49s  (93.0%)65.4%  lr: 0.003564  loss: 0.001356  eta: 0h1m  tot: 0h13m49s  (93.1%)65.8%  lr: 0.003524  loss: 0.001356  eta: 0h1m  tot: 0h13m50s  (93.2%)66.0%  lr: 0.003494  loss: 0.001356  eta: 0h1m  tot: 0h13m50s  (93.2%)67.1%  lr: 0.003373  loss: 0.001359  eta: <1min   tot: 0h13m52s  (93.4%)68.0%  lr: 0.003273  loss: 0.001361  eta: <1min   tot: 0h13m53s  (93.6%)68.6%  lr: 0.003203  loss: 0.001358  eta: <1min   tot: 0h13m55s  (93.7%)69.0%  lr: 0.003173  loss: 0.001359  eta: <1min   tot: 0h13m56s  (93.8%)%  lr: 0.003163  loss: 0.001356  eta: <1min   tot: 0h13m56s  (93.9%)71.0%  lr: 0.002943  loss: 0.001356  eta: <1min   tot: 0h13m59s  (94.2%)71.9%  lr: 0.002883  loss: 0.001355  eta: <1min   tot: 0h14m1s  (94.4%)72.4%  lr: 0.002813  loss: 0.001354  eta: <1min   tot: 0h14m2s  (94.5%)73.7%  lr: 0.002723  loss: 0.001361  eta: <1min   tot: 0h14m4s  (94.7%)74.0%  lr: 0.002673  loss: 0.001359  eta: <1min   tot: 0h14m4s  (94.8%)74.2%  lr: 0.002663  loss: 0.001360  eta: <1min   tot: 0h14m5s  (94.8%)74.8%  lr: 0.002563  loss: 0.001359  eta: <1min   tot: 0h14m6s  (95.0%)75.4%  lr: 0.002533  loss: 0.001358  eta: <1min   tot: 0h14m6s  (95.1%)75.7%  lr: 0.002533  loss: 0.001359  eta: <1min   tot: 0h14m7s  (95.1%)  eta: <1min   tot: 0h14m9s  (95.4%)77.1%  lr: 0.002412  loss: 0.001359  eta: <1min   tot: 0h14m9s  (95.4%)77.3%  lr: 0.002402  loss: 0.001361  eta: <1min   tot: 0h14m9s  (95.5%)77.5%  lr: 0.002402  loss: 0.001362  eta: <1min   tot: 0h14m10s  (95.5%)77.6%  lr: 0.002392  loss: 0.001361  eta: <1min   tot: 0h14m10s  (95.5%)78.7%  lr: 0.002282  loss: 0.001363  eta: <1min   tot: 0h14m11s  (95.7%)79.0%  lr: 0.002222  loss: 0.001364  eta: <1min   tot: 0h14m12s  (95.8%)79.1%  lr: 0.002222  loss: 0.001363  eta: <1min   tot: 0h14m12s  (95.8%)79.5%  lr: 0.002212  loss: 0.001361  eta: <1min   tot: 0h14m13s  (95.9%)79.9%  lr: 0.002172  loss: 0.001360  eta: <1min   tot: 0h14m13s  (96.0%)80.2%  lr: 0.002162  loss: 0.001357  eta: <1min   tot: 0h14m14s  (96.0%)%  lr: 0.002162  loss: 0.001356  eta: <1min   tot: 0h14m14s  (96.1%)81.3%  lr: 0.002102  loss: 0.001356  eta: <1min   tot: 0h14m15s  (96.3%)82.0%  lr: 0.002082  loss: 0.001356  eta: <1min   tot: 0h14m16s  (96.4%)82.1%  lr: 0.002082  loss: 0.001356  eta: <1min   tot: 0h14m16s  (96.4%)%  lr: 0.002072  loss: 0.001355  eta: <1min   tot: 0h14m16s  (96.4%)82.3%  lr: 0.002072  loss: 0.001354  eta: <1min   tot: 0h14m17s  (96.5%)  loss: 0.001356  eta: <1min   tot: 0h14m17s  (96.5%)82.9%  lr: 0.001982  loss: 0.001355  eta: <1min   tot: 0h14m18s  (96.6%)83.5%  lr: 0.001912  loss: 0.001355  eta: <1min   tot: 0h14m18s  (96.7%)83.5%  lr: 0.001912  loss: 0.001355  eta: <1min   tot: 0h14m18s  (96.7%)84.8%  lr: 0.001742  loss: 0.001361  eta: <1min   tot: 0h14m20s  (97.0%)85.0%  lr: 0.001722  loss: 0.001360  eta: <1min   tot: 0h14m21s  (97.0%)85.2%  lr: 0.001702  loss: 0.001362  eta: <1min   tot: 0h14m21s  (97.0%)86.2%  lr: 0.001622  loss: 0.001364  eta: <1min   tot: 0h14m23s  (97.2%)86.6%  lr: 0.001552  loss: 0.001362  eta: <1min   tot: 0h14m24s  (97.3%)86.7%  lr: 0.001552  loss: 0.001361  eta: <1min   tot: 0h14m24s  (97.3%)86.9%  lr: 0.001542  loss: 0.001360  eta: <1min   tot: 0h14m24s  (97.4%)87.1%  lr: 0.001522  loss: 0.001359  eta: <1min   tot: 0h14m25s  (97.4%)87.5%  lr: 0.001492  loss: 0.001360  eta: <1min   tot: 0h14m25s  (97.5%)89.2%  lr: 0.001251  loss: 0.001361  eta: <1min   tot: 0h14m28s  (97.8%)m29s  (98.0%)89.8%  lr: 0.001141  loss: 0.001362  eta: <1min   tot: 0h14m29s  (98.0%)90.2%  lr: 0.001121  loss: 0.001361  eta: <1min   tot: 0h14m30s  (98.0%)90.3%  lr: 0.001111  loss: 0.001361  eta: <1min   tot: 0h14m30s  (98.1%)%  lr: 0.001101  loss: 0.001360  eta: <1min   tot: 0h14m30s  (98.1%)  lr: 0.001061  loss: 0.001359  eta: <1min   tot: 0h14m31s  (98.2%)91.2%  lr: 0.001051  loss: 0.001359  eta: <1min   tot: 0h14m32s  (98.2%)91.7%  lr: 0.001001  loss: 0.001359  eta: <1min   tot: 0h14m32s  (98.3%)93.4%  lr: 0.000861  loss: 0.001360  eta: <1min   tot: 0h14m35s  (98.7%)94.4%  lr: 0.000811  loss: 0.001359  eta: <1min   tot: 0h14m36s  (98.9%)94.5%  lr: 0.000791  loss: 0.001359  eta: <1min   tot: 0h14m36s  (98.9%)  lr: 0.000781  loss: 0.001361  eta: <1min   tot: 0h14m36s  (98.9%)95.7%  lr: 0.000711  loss: 0.001360  eta: <1min   tot: 0h14m38s  (99.1%)95.8%  lr: 0.000691  loss: 0.001360  eta: <1min   tot: 0h14m38s  (99.2%)95.9%  lr: 0.000661  loss: 0.001360  eta: <1min   tot: 0h14m38s  (99.2%)96.9%  lr: 0.000601  loss: 0.001360  eta: <1min   tot: 0h14m40s  (99.4%)97.5%  lr: 0.000541  loss: 0.001361  eta: <1min   tot: 0h14m41s  (99.5%)97.6%  lr: 0.000531  loss: 0.001361  eta: <1min   tot: 0h14m41s  (99.5%)97.9%  lr: 0.000491  loss: 0.001359  eta: <1min   tot: 0h14m42s  (99.6%)98.1%  lr: 0.000471  loss: 0.001359  eta: <1min   tot: 0h14m42s  (99.6%)98.8%  lr: 0.000380  loss: 0.001364  eta: <1min   tot: 0h14m43s  (99.8%)99.0%  lr: 0.000340  loss: 0.001363  eta: <1min   tot: 0h14m43s  (99.8%)99.1%  lr: 0.000340  loss: 0.001363  eta: <1min   tot: 0h14m43s  (99.8%)99.5%  lr: 0.000290  loss: 0.001362  eta: <1min   tot: 0h14m44s  (99.9%)\n",
      " ---+++                Epoch    4 Train error : 0.00138711 +++--- ���\n",
      "Saving model to file : questionspace\n",
      "Saving model in tsv format : questionspace.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile data/train_prepared.tsv -model questionspace -trainMode 3 -adagrad True -ngrams 1 -epoch 5 -dim 100 -similarity 'cosine' -minCount 2 -verbose True -fileFormat 'labelDoc' -negSearchLimit 10 -lr 0.05    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "lines = [line.strip().split('\\t') for line in open('questionspace.tsv')]\n",
    "starspace_embeddings = {i[0]: np.array(i[1:], dtype=float) for i in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.517 | Hits@   1: 0.517\n",
      "DCG@   5: 0.614 | Hits@   5: 0.698\n",
      "DCG@  10: 0.634 | Hits@  10: 0.758\n",
      "DCG@ 100: 0.664 | Hits@ 100: 0.904\n",
      "DCG@ 500: 0.674 | Hits@ 500: 0.982\n",
      "DCG@1000: 0.676 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 91\t28\t61\t48\t13\t98\t78\t59\t21\t84\t4\t1\t14\t36\t96\t46\t2\t66\t30\t90\t99\t82\t25\t97\t83\t26\t64\t76\t67\t10\t50\t55\t88\t43\t8...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "######### YOUR CODE HERE #############\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857287\n",
      "0.5\n",
      "0.8154648767857287\n",
      "0.3333333333333333\n",
      "0.5436432511904857\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 91\t28\t61\t48\t13\t98\t78\t59\t21\t84\t4\t1\t14\t36\t96\t46\t2\t66\t30\t90\t99\t82\t25\t97\t83\t26\t64\t76\t67\t10\t50\t55\t88\t43\t8...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'learning@leenamurgai.co.uk' # EMAIL \n",
    "STUDENT_TOKEN = 'h0WGquVuYsr4EbHe' # TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
